{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uF25NSM2jQxv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "omk-SitljQx0"
      },
      "outputs": [],
      "source": [
        "%cd -q .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrwcspd8jQx2",
        "outputId": "1e115eea-7205-4e8d-884d-9a35e1ffe9f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/antonio14bernardes/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.2+cu121\n"
          ]
        }
      ],
      "source": [
        "import lcpfn\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhkhXEvjQx3"
      },
      "source": [
        "# Training the LCPFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "_nHnLBlNjQx5",
        "outputId": "2778df34-4cdf-429c-d40f-cb503fbf2e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.11.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "93060607dd4b4fa1a488c8ba00144b21",
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install torch==1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt2BrGFbjQx7",
        "outputId": "776f3d41-55fb-487d-a30d-8d76a49a0834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 1000000 y evals to estimate 1000 buckets. Cut off the last 0 ys.\n",
            "Using cuda:0 device\n",
            "init dist\n",
            "Not using distributed\n",
            "DataLoader.__dict__ {'num_steps': 100, 'get_batch_kwargs': {'batch_size': 500, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7eec9fe0f0a0>, 'seq_len_maximum': 100, 'device': 'cuda:0', 'num_features': 1, 'hyperparameters': {}}, 'num_features': 1}\n",
            "Style definition: None\n",
            "Using a Transformer with 2.23 M parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/lcpfn/lcpfn/bar_distribution.py:22: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at  ../aten/src/ATen/native/BucketizationUtils.h:28.)\n",
            "  target_sample = torch.searchsorted(self.borders, y) - 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 46.40s | mean loss -0.07 | pos losses   nan,-0.06,-0.06,-0.05,  nan,  nan,-0.06,-0.06,-0.07,-0.06,  nan,-0.13,  nan,-0.10,-0.04,-0.06,  nan,-0.07,-0.07,-0.06,-0.05,-0.06,-0.08,  nan,-0.08,-0.09,-0.04,-0.07,-0.07,  nan,-0.10,-0.03,-0.07,  nan,-0.05,-0.06,  nan,-0.04,-0.07,-0.07,-0.08,  nan,  nan,  nan,-0.04,-0.07,-0.06,-0.07,  nan,-0.05,  nan,-0.08,-0.07,-0.07,  nan,-0.05,-0.06,  nan,-0.07,-0.05,  nan,  nan,-0.06,-0.08,-0.05,-0.09,-0.06,-0.05,-0.07,  nan,-0.09,-0.09,  nan,-0.06,  nan,  nan,  nan,  nan,  nan,-0.06,  nan,  nan,  nan,-0.05,-0.07,-0.06,-0.08,-0.08,-0.06,-0.04,  nan,  nan,  nan,-0.04,  nan,-0.06,  nan,-0.06,  nan,-0.08, lr 0.0 data time  0.33 step time  0.10 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 47.84s | mean loss -0.29 | pos losses   nan,  nan,-0.25,-0.29,  nan,  nan,-0.34,-0.29,-0.55,-0.54,-0.31,-0.09,-0.36,-0.10,  nan,  nan,-0.06,-0.55,  nan,-0.13,-0.20,-0.33,  nan,  nan,-0.53,-0.19,  nan,-0.09,  nan,-0.12,  nan,-0.02,  nan,  nan,  nan,  nan,-0.36,-0.08,  nan,-0.21,  nan,-0.31,  nan,-0.38,-0.20,  nan,-0.34,-0.53,-0.37,  nan,-0.64,  nan,-0.51,  nan,-0.18,  nan,  nan,  nan,  nan,-0.24,-0.53,-0.11,-0.33,-0.22,  nan,-0.14,  nan,-0.40,-0.38,-0.42,-0.09,  nan,  nan,-0.31,  nan,-0.05,-0.27,  nan,  nan,-0.30,  nan,-0.26,  nan,-0.07,  nan,-0.65,  nan,-0.69,  nan,-0.26,-0.53,-0.39,-0.54,-0.30,-0.39,  nan,-0.23,  nan,-0.23,  nan, lr 4e-05 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 48.15s | mean loss -1.08 | pos losses   nan, 0.05,-0.49,-0.57,  nan,  nan,-0.97,  nan,-0.96,-0.91,-0.81,  nan,  nan,-1.09,-0.95,-1.09,  nan,-0.93,-1.06,  nan,  nan,-1.18,  nan,-0.84,  nan,-1.03,-1.21,-1.01,-0.66,-1.17,  nan,-1.30,-0.82,-1.03,-1.20,-0.95,-0.78,-1.13,  nan,-0.84,-1.05,-1.38,  nan,  nan,-1.38,  nan,-1.24,-0.98,-1.08,  nan,  nan,-1.17,-1.13,-1.25,  nan,-1.43,-1.28,  nan,-1.35,-1.01,-1.09,-1.32,  nan,-1.28,  nan,  nan,-1.36,-1.27,-1.02,  nan,  nan,-1.12,-1.08,  nan,-1.46,  nan,-1.33,  nan,-1.09,-1.30,-1.31,-1.00,-0.72,-1.15,-0.79,-1.11,-1.27,-1.13,  nan,  nan,-1.23,-0.76,  nan,  nan,  nan,  nan,  nan,  nan,-0.91,-1.19, lr 8e-05 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 48.36s | mean loss -1.45 | pos losses   nan, 0.26,  nan, 0.01,-0.35,-0.65,-0.93,  nan,-1.02,  nan,  nan,  nan,-1.23,-1.32,  nan,  nan,  nan,  nan,-1.46,-1.35,-1.45,  nan,-1.38,-1.48,-1.32,-1.52,-1.34,-1.47,-1.44,-1.33,  nan,-1.53,  nan,-1.47,-1.53,-1.43,-1.51,  nan,  nan,-1.63,-1.55,-1.48,  nan,  nan,  nan,  nan,  nan,  nan,-1.60,-1.54,-1.62,-1.67,-1.58,-1.53,-1.50,-1.60,  nan,-1.63,-1.45,  nan,-1.62,-1.64,  nan,-1.51,-1.48,  nan,  nan,-1.62,-1.49,-1.65,  nan,  nan,-1.54,-1.54,-1.46,-1.56,  nan,-1.53,-1.68,-1.49,-1.65,  nan,  nan,-1.63,  nan,-1.44,  nan,  nan,  nan,-1.56,-1.66,-1.49,-1.57,  nan,-1.70,-1.58,  nan,-1.68,-1.57,-1.64, lr 0.00012 data time  0.30 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 48.75s | mean loss -1.50 | pos losses   nan, 1.35, 0.39,  nan,  nan,-0.67,  nan,-0.74,  nan,  nan,-1.20,-1.19,-1.32,-1.29,  nan,-1.27,  nan,-1.46,-1.45,-1.44,-1.37,-1.46,-1.52,  nan,  nan,  nan,  nan,-1.50,-1.64,  nan,-1.61,-1.61,  nan,-1.50,-1.62,  nan,  nan,  nan,-1.70,  nan,  nan,-1.66,-1.71,-1.71,  nan,-1.70,  nan,-1.72,-1.69,  nan,-1.64,  nan,-1.76,  nan,  nan,-1.73,-1.73,-1.72,-1.61,-1.69,-1.70,-1.73,-1.68,-1.66,-1.69,  nan,-1.77,-1.72,-1.69,-1.72,-1.69,-1.66,  nan,  nan,-1.69,  nan,  nan,-1.70,-1.69,-1.71,-1.63,  nan,  nan,  nan,-1.62,  nan,-1.75,  nan,-1.65,  nan,-1.81,-1.73,-1.65,-1.72,  nan,-1.64,-1.72,-1.79,-1.78,-1.78, lr 0.00016 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 49.59s | mean loss -1.47 | pos losses   nan, 0.74, 0.31,-0.43,  nan,  nan,-0.87,-0.93,  nan,-1.11,-1.01,-1.11,-1.26,-1.35,-1.40,-1.39,  nan,-1.45,  nan,-1.41,-1.46,-1.40,-1.54,  nan,-1.53,-1.57,  nan,-1.57,-1.54,-1.61,-1.60,  nan,-1.68,-1.59,-1.62,  nan,-1.59,-1.61,  nan,  nan,  nan,  nan,  nan,  nan,-1.66,-1.65,-1.68,  nan,-1.74,-1.72,  nan,-1.70,-1.64,  nan,  nan,  nan,-1.74,  nan,  nan,-1.62,-1.65,  nan,  nan,  nan,-1.76,-1.66,  nan,-1.73,-1.67,  nan,  nan,-1.79,  nan,  nan,-1.76,-1.61,-1.66,-1.78,  nan,-1.68,-1.70,-1.74,-1.78,-1.69,-1.75,  nan,-1.49,  nan,-1.71,  nan,-1.71,  nan,-1.76,-1.71,-1.69,  nan,-1.61,-1.65,  nan,-1.67, lr 0.0002 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 49.32s | mean loss -1.49 | pos losses   nan, 0.74,-0.14,-0.49,-0.79,  nan,-1.00,-0.81,-1.08,-1.08,  nan,  nan,-1.14,  nan,-1.40,-1.47,  nan,-1.33,-1.37,-1.53,-1.51,  nan,  nan,-1.41,-1.43,  nan,-1.46,  nan,-1.55,-1.59,-1.70,  nan,-1.58,  nan,  nan,-1.65,-1.68,-1.71,-1.66,-1.54,-1.70,-1.67,-1.49,  nan,-1.67,-1.70,-1.79,  nan,  nan,-1.71,  nan,  nan,-1.73,-1.71,  nan,  nan,  nan,  nan,-1.62,-1.73,-1.07,-1.74,-1.70,-1.73,  nan,  nan,-1.61,-1.80,  nan,  nan,-1.72,-1.78,  nan,  nan,-1.69,  nan,-1.78,  nan,-1.66,-1.75,  nan,-1.73,-1.65,-1.82,  nan,-1.58,-1.85,  nan,-1.80,  nan,  nan,  nan,-1.79,-1.82,-1.68,-1.69,-1.31,-1.76,-1.42,  nan, lr 0.00024 data time  0.23 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 49.22s | mean loss -1.70 | pos losses   nan,  nan,  nan,-0.57,  nan,-0.87,  nan,-1.13,-1.11,-1.06,-1.34,  nan,-1.34,  nan,-1.49,  nan,-1.46,  nan,-1.56,-1.51,-1.60,-1.66,  nan,  nan,-1.72,-1.66,  nan,-1.72,-1.73,-1.73,  nan,-1.70,  nan,-1.75,-1.63,-1.69,  nan,  nan,-1.76,-1.74,-1.75,-1.76,  nan,  nan,  nan,-1.76,-1.75,-1.85,  nan,  nan,-1.74,-1.82,-1.85,-1.80,  nan,  nan,-1.81,  nan,  nan,-1.67,  nan,  nan,-1.91,-1.74,-1.86,-1.82,-1.64,-1.88,-1.84,-1.85,-1.88,-1.86,  nan,-1.84,-1.88,  nan,-1.63,  nan,  nan,  nan,-1.73,-1.79,  nan,  nan,-1.84,  nan,  nan,-1.81,-1.79,-1.83,-1.78,-1.94,  nan,-1.85,-1.87,-1.92,  nan,-1.94,  nan,-1.89, lr 0.00028000000000000003 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 50.16s | mean loss -1.58 | pos losses   nan, 0.78,-0.55,-0.63,-0.84,  nan,-0.98,-1.13,-1.11,-1.25,-1.21,-1.34,-1.44,  nan,-1.28,-1.43,-1.37,-1.54,-1.42,  nan,-1.69,  nan,-1.64,  nan,-1.64,  nan,-1.60,-1.68,-1.68,  nan,  nan,  nan,-1.56,  nan,-1.79,  nan,  nan,-1.73,  nan,-1.72,-1.81,  nan,-1.80,  nan,-1.85,-1.81,  nan,  nan,  nan,  nan,-1.89,-1.50,  nan,-1.89,-1.82,-1.86,  nan,-1.84,  nan,-1.72,  nan,-1.40,-1.92,  nan,-1.87,-1.96,-1.58,-1.62,-1.91,-1.89,  nan,-1.90,-1.89,  nan,-1.37,-1.91,  nan,  nan,-1.93,  nan,  nan,-1.55,  nan,  nan,-1.80,-1.71,-1.91,  nan,  nan,  nan,-1.76,  nan,-1.63,-1.47,-2.02,  nan,-1.94,-1.93,  nan,-1.89, lr 0.00032 data time  0.16 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 50.20s | mean loss -1.68 | pos losses   nan, 0.42,-0.67,  nan,-0.98,-0.92,-1.08,-1.28,-1.21,  nan,  nan,-1.42,-1.42,-1.46,  nan,-1.53,-1.43,-1.56,-1.64,-1.55,-1.67,  nan,-1.63,-1.50,  nan,  nan,-1.74,-1.79,-1.68,-1.81,  nan,  nan,-1.54,  nan,-1.81,-1.80,-1.73,-1.75,-1.86,-1.85,-1.72,-1.90,-1.84,-1.83,  nan,-1.81,  nan,-1.90,-1.93,  nan,  nan,  nan,  nan,-1.90,-1.96,-1.90,-1.95,-1.79,  nan,-1.86,-1.92,-1.89,  nan,-1.85,  nan,  nan,-1.87,-1.96,-1.87,  nan,  nan,-1.79,  nan,  nan,-1.94,  nan,-2.00,-1.94,  nan,-1.85,  nan,-1.91,  nan,  nan,  nan,  nan,-1.52,-1.77,-1.87,  nan,-2.00,-1.97,  nan,-1.98,  nan,-1.91,-2.00,-1.92,-1.88,-1.90, lr 0.00035999999999999997 data time  0.22 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 49.85s | mean loss -1.73 | pos losses   nan, 0.40,  nan,  nan,  nan,-0.97,  nan,-1.13,  nan,-1.27,-1.14,-1.25,-1.47,  nan,-1.45,-1.57,-1.23,-1.45,-1.53,  nan,-1.64,  nan,  nan,  nan,-1.66,  nan,-1.53,-1.70,-1.62,-1.73,-1.56,-1.79,-1.79,-1.80,-1.83,-1.64,-1.87,  nan,  nan,-1.87,-1.84,-1.92,-1.89,  nan,-1.91,-1.80,-1.95,-1.97,  nan,-1.86,-1.93,-1.94,-1.89,  nan,-1.93,  nan,-1.74,-1.64,  nan,  nan,  nan,-1.95,  nan,-1.94,  nan,-1.99,  nan,  nan,-1.94,-2.03,-1.96,-1.67,-2.04,  nan,-1.97,  nan,-2.00,  nan,  nan,-2.03,-1.91,-2.00,-2.07,-2.03,  nan,-1.91,-2.02,  nan,-1.83,  nan,  nan,  nan,  nan,  nan,-2.06,-2.03,-1.87,-1.95,  nan,-2.03, lr 0.0004 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 49.57s | mean loss -1.73 | pos losses   nan, 0.71,-0.55,  nan,  nan,  nan,  nan,-1.03,-1.24,-1.26,-1.32,  nan,  nan,-1.44,-1.49,-1.49,  nan,  nan,-1.41,-1.58,-1.60,  nan,-1.70,-1.48,  nan,-1.58,  nan,  nan,-1.74,-1.66,  nan,  nan,-1.76,-1.88,  nan,-1.62,-1.77,  nan,  nan,-1.78,-1.88,  nan,  nan,  nan,-1.87,  nan,  nan,  nan,  nan,-1.77,-1.99,-1.96,-1.90,-1.95,-1.97,-1.90,  nan,-1.97,-1.77,-1.87,  nan,-1.98,-1.93,-1.88,-1.84,-1.93,-1.85,-2.04,-1.61,  nan,-1.81,  nan,  nan,-1.70,-2.06,-1.95,-2.03,-2.06,-1.95,-1.62,-1.79,-1.82,-1.67,-1.78,-1.25,-1.97,  nan,-2.01,  nan,-1.80,-1.84,-2.01,-1.84,  nan,  nan,-1.92,-1.93,-2.12,  nan,-2.00, lr 0.00044 data time  0.30 step time  0.12 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 49.85s | mean loss -1.70 | pos losses   nan,  nan,-0.35,-0.73,-0.67,-1.03,-1.11,  nan,  nan,-1.15,-1.33,  nan,-1.46,-1.46,  nan,-1.52,  nan,-1.47,-1.10,  nan,-1.55,  nan,-1.63,-1.53,-1.69,  nan,-1.74,  nan,  nan,-1.74,-1.77,  nan,  nan,-1.95,-1.79,-0.97,-1.83,-1.84,-1.82,  nan,-1.82,-1.89,-1.87,-1.82,-1.67,-1.93,-1.89,-2.02,-1.68,  nan,-1.99,  nan,-1.99,-1.80,-1.99,-1.94,-2.00,  nan,  nan,-1.97,  nan,  nan,  nan,  nan,-1.92,  nan,-2.02,  nan,-1.84,  nan,  nan,  nan,  nan,  nan,-2.13,  nan,  nan,  nan,-1.99,-2.07,-1.53,  nan,-1.93,  nan,-1.91,-1.93,-1.95,  nan,-2.03,  nan,  nan,-2.00,-1.94,-2.03,-1.99,  nan,  nan,  nan,-1.89,-1.94, lr 0.00048 data time  0.20 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 50.14s | mean loss -1.65 | pos losses   nan, 0.49,  nan,-0.68,-0.90,  nan,-1.08,-1.18,-1.20,-1.04,  nan,-0.96,  nan,-1.36,  nan,-1.45,-1.53,-1.44,-1.52,  nan,  nan,-1.63,-1.57,  nan,  nan,-1.61,-1.70,  nan,  nan,-1.64,  nan,  nan,-1.77,  nan,-1.82,-1.60,  nan,-1.57,-1.80,-1.65,-1.78,-1.94,-1.84,  nan,-1.87,  nan,-1.77,-1.54,-1.89,-1.90,-1.92,-1.91,-1.92,-1.72,  nan,-2.00,  nan,  nan,  nan,  nan,  nan,  nan,-1.99,-1.81,-1.82,-1.88,  nan,-1.97,  nan,-1.84,  nan,  nan,-1.89,-1.05,  nan,-1.90,  nan,  nan,  nan,  nan,  nan,-1.89,  nan,-2.02,  nan,-0.99,  nan,-1.63,-1.47,-2.06,  nan,-2.06,  nan,-2.04,  nan,-1.90,-1.81,-2.04,  nan,-1.70, lr 0.0005200000000000001 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 49.89s | mean loss -1.79 | pos losses   nan,  nan,  nan,-0.81,-0.82,-0.98,  nan,  nan,-1.29,-1.28,  nan,  nan,-1.17,-1.42,-1.47,-1.45,  nan,-1.53,-1.58,-1.54,-1.67,  nan,-1.68,-1.63,  nan,  nan,-1.83,  nan,  nan,  nan,-1.79,-1.73,-1.73,-1.79,-1.84,  nan,  nan,-1.88,-1.88,  nan,  nan,-1.95,  nan,-1.95,-1.93,  nan,  nan,-1.96,  nan,-1.95,-1.88,-1.97,  nan,-2.06,-1.98,-1.92,-2.07,  nan,  nan,-1.84,-1.91,-2.05,-1.95,  nan,-1.95,-1.53,-1.82,  nan,-1.94,-2.04,-1.95,-2.00,-1.94,  nan,  nan,-1.94,  nan,  nan,-2.03,  nan,-2.03,  nan,-1.95,  nan,-1.92,-2.06,-1.95,-1.97,-1.88,-2.03,  nan,-1.95,-1.55,  nan,-1.97,-1.92,  nan,  nan,-0.62,-1.76, lr 0.0005600000000000001 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 50.05s | mean loss -1.76 | pos losses   nan,  nan,-0.73,-0.84,-0.85,  nan,  nan,-1.27,-1.20,  nan,  nan,-1.50,-1.50,  nan,-1.44,-1.47,-1.65,-1.59,-1.61,-1.50,-1.55,-1.76,-1.79,-1.77,-1.63,-1.73,-1.54,  nan,  nan,-1.84,  nan,-1.92,-1.77,-1.90,  nan,  nan,-1.66,-1.86,  nan,-1.90,-1.95,-1.86,  nan,  nan,  nan,-1.55,-1.91,-1.92,-1.89,-1.84,-1.96,  nan,  nan,-1.94,  nan,  nan,-2.01,  nan,-2.03,  nan,-1.92,  nan,-1.67,  nan,  nan,-2.04,-2.03,  nan,-1.94,-2.01,  nan,  nan,  nan,-1.97,-2.04,-1.99,-1.94,-1.88,-1.99,  nan,  nan,-2.11,-1.44,-2.02,  nan,-2.04,-2.08,-2.05,  nan,-1.66,  nan,  nan,  nan,  nan,  nan,-1.83,  nan,  nan,-1.92,  nan, lr 0.0006 data time  0.26 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 49.16s | mean loss -1.80 | pos losses   nan, 0.32,  nan,  nan,-0.83,  nan,-0.99,-1.20,-0.37,-1.26,-1.40,-1.39,-1.43,  nan,-1.38,-1.70,-1.57,-1.58,-1.40,-1.66,  nan,  nan,-1.66,  nan,-1.84,-1.82,  nan,-1.83,  nan,-1.81,  nan,-1.90,-1.70,-1.69,  nan,  nan,-1.87,-1.91,-1.88,  nan,  nan,-1.86,-1.85,  nan,  nan,-1.85,-1.76,-1.88,-1.92,-1.97,-2.05,-1.90,  nan,-1.73,  nan,-2.02,-2.04,-2.01,-2.11,-2.05,  nan,-2.10,-2.00,-2.10,-2.02,-2.00,-1.91,  nan,-2.10,  nan,-1.99,  nan,  nan,  nan,-2.01,-2.09,-2.00,-1.96,  nan,-1.90,-2.13,  nan,  nan,-2.05,-1.92,-1.99,  nan,  nan,-2.00,-1.89,-2.06,-2.17,  nan,-1.95,-1.49,  nan,-2.03,-1.90,  nan,-1.92, lr 0.00064 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 50.30s | mean loss -1.79 | pos losses   nan,  nan,-0.57,  nan,-0.86,-1.05,-1.04,  nan,-0.96,-1.34,  nan,-1.42,-1.47,-1.51,-1.54,  nan,-1.61,-1.56,-1.62,-1.68,-1.64,  nan,-1.85,  nan,  nan,  nan,-1.72,-1.71,  nan,-1.78,-1.75,  nan,-1.91,  nan,-1.77,-1.86,-1.77,  nan,-1.80,-1.91,  nan,-1.97,  nan,-2.04,-1.91,-1.98,-1.87,-1.94,-1.96,  nan,-1.98,-1.97,-1.96,  nan,  nan,-1.91,-1.98,-1.82,  nan,-2.02,-1.98,-2.04,-2.07,-2.10,-1.96,-1.80,-1.78,-1.91,  nan,-1.89,  nan,  nan,  nan,-1.88,  nan,  nan,-1.91,-2.07,-1.45,  nan,-2.01,-2.10,  nan,-1.89,-1.88,-1.66,-2.10,  nan,-2.02,-2.07,-1.79,  nan,  nan,-1.81,-2.05,-1.99,-2.07,-1.95,-1.97,-2.03, lr 0.00068 data time  0.19 step time  0.11 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 50.27s | mean loss -1.58 | pos losses   nan, 0.94,-0.56,  nan,-0.66,-0.84,-0.87,-1.11,-1.26,-0.96,-1.32,  nan,-1.28,-1.58,  nan,-1.45,  nan,-1.56,  nan,-1.63,-1.60,-1.69,-1.69,-1.54,-1.74,-1.51,-1.53,-1.68,-1.77,  nan,-1.86,  nan,  nan,-1.89,  nan,  nan,-1.71,-1.76,  nan,-1.91,  nan,  nan,-1.82,  nan,  nan,  nan,-1.71,  nan,  nan,  nan,  nan,-1.80,  nan,-1.93,  nan,-2.01,  nan,-1.93,-1.89,  nan,  nan,  nan,  nan,-1.54,-2.00,-1.80,-1.83,-1.88,-1.37,  nan,  nan,-1.93,  nan,  nan,-1.99,-1.85,  nan,-1.78,-1.96,  nan,  nan,-1.98,-2.00,-1.82,  nan,-1.69,-1.92,-1.69,  nan,-1.82,-1.42,-1.91,-1.77,-2.04,  nan,-1.93,-2.09,  nan,-1.54,-1.77, lr 0.0007199999999999999 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 50.15s | mean loss -1.74 | pos losses   nan,  nan,-0.59,-0.86,-0.93,-1.05,  nan,-1.16,-1.19,-1.23,  nan,  nan,-1.43,-1.12,-1.52,-1.49,-1.57,  nan,-1.22,  nan,-1.63,  nan,-1.65,-1.67,-1.74,-1.64,  nan,-1.76,-1.65,-1.47,  nan,-1.84,  nan,  nan,  nan,  nan,-1.62,-1.85,  nan,-1.87,  nan,-1.93,  nan,  nan,  nan,  nan,  nan,-1.88,  nan,-1.97,-1.68,  nan,-1.95,  nan,  nan,-1.91,-1.97,  nan,-1.55,  nan,  nan,-2.08,  nan,-1.62,-1.98,  nan,-2.00,-2.02,-1.98,-1.75,-1.93,  nan,-1.98,-2.04,-2.05,-1.91,-1.94,-1.99,-2.13,-2.00,-1.70,-1.98,-2.05,  nan,  nan,-2.03,-2.00,  nan,-2.10,-1.79,-2.16,-2.05,  nan,-1.89,  nan,-1.99,-2.07,-1.75,-1.93,-2.11, lr 0.00076 data time  0.21 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 49.80s | mean loss -1.79 | pos losses   nan,  nan,-0.43,-0.83,  nan,-0.92,-1.10,-1.07,  nan,-1.30,-1.37,-1.33,  nan,-1.55,-1.43,-1.59,  nan,  nan,  nan,-1.60,-1.67,-1.82,  nan,-1.69,-1.82,-1.85,-1.81,-1.84,-1.74,-1.69,  nan,-1.80,  nan,  nan,  nan,  nan,-1.81,  nan,-1.97,-1.74,-1.90,-2.06,-1.95,-2.00,  nan,  nan,  nan,  nan,-2.10,-1.98,-1.88,  nan,-1.93,-2.00,-1.95,-1.86,  nan,-1.95,-2.08,-2.08,-2.01,-1.55,  nan,-2.04,  nan,-1.97,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.07,-2.17,-1.69,-2.07,-2.10,  nan,  nan,-2.11,-1.81,-1.94,  nan,-2.10,-2.10,-2.11,-1.94,-1.98,  nan,-2.07,-2.07,-2.07,-2.21,  nan,-2.10,  nan,  nan,  nan,-2.05, lr 0.0008 data time  0.29 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 49.51s | mean loss -1.72 | pos losses   nan, 0.61,-0.15,-0.67,-0.82,  nan,  nan,-1.09,-1.18,-0.81,  nan,-1.28,  nan,-1.27,-1.38,-1.08,-1.35,  nan,  nan,-1.61,-1.70,-1.68,  nan,-1.72,-1.79,-1.58,-1.72,-1.82,  nan,  nan,-1.71,-1.91,  nan,  nan,-1.79,-1.72,  nan,-1.85,  nan,-1.98,-1.72,-1.92,  nan,  nan,  nan,-2.05,-1.91,-1.85,-1.58,-1.91,-2.03,-1.94,-1.75,-1.96,-1.87,-1.95,  nan,-1.99,-1.99,-2.09,-1.92,  nan,-1.90,-1.98,-2.04,-2.11,  nan,  nan,-2.00,-1.68,-1.91,  nan,  nan,-1.89,  nan,-2.00,-1.71,-1.45,  nan,  nan,-2.10,  nan,  nan,-2.00,  nan,  nan,-1.42,  nan,  nan,-1.85,-1.91,-1.78,  nan,-1.91,-1.72,-1.90,-1.96,-2.02,  nan,  nan, lr 0.00084 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 49.95s | mean loss -1.81 | pos losses   nan,  nan,-0.58,-0.81,  nan,  nan,  nan,-1.11,  nan,-1.29,-1.42,-1.44,-1.35,-1.47,-1.51,-1.69,-1.56,-1.65,-1.69,  nan,-1.56,-1.53,-1.66,-1.78,  nan,  nan,-1.82,-1.89,-1.87,  nan,-1.81,  nan,-1.92,  nan,-1.97,-1.87,-1.94,  nan,  nan,  nan,-2.02,-1.99,-1.95,-1.95,  nan,-1.98,  nan,  nan,-1.78,-1.98,-2.01,-1.81,  nan,-2.05,  nan,  nan,-2.04,  nan,-2.13,-2.07,  nan,-1.96,-1.89,  nan,-1.98,  nan,-2.05,-1.95,  nan,-2.01,  nan,  nan,-1.96,  nan,  nan,  nan,  nan,-2.05,-1.79,-2.07,-2.01,-2.10,  nan,-2.02,-2.06,  nan,-2.08,-1.88,-1.96,-2.10,-1.96,  nan,-2.06,-1.95,  nan,  nan,-2.05,  nan,-2.02,-2.18, lr 0.00088 data time  0.17 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 50.17s | mean loss -1.71 | pos losses   nan, 0.01,  nan,  nan,-0.72,  nan,  nan,-1.19,  nan,-1.35,-1.32,  nan,-1.32,-1.44,  nan,-1.47,-1.50,-1.53,  nan,  nan,  nan,  nan,  nan,-1.54,  nan,-1.75,-1.75,-1.54,  nan,-1.81,-1.87,-1.84,  nan,  nan,  nan,  nan,-1.87,-1.60,  nan,-1.95,  nan,  nan,-1.72,-1.78,  nan,  nan,  nan,  nan,-1.50,-1.84,-1.86,  nan,-1.89,-0.63,  nan,  nan,-1.88,-1.98,-1.91,-1.92,-2.00,  nan,-1.82,-2.08,-2.13,-1.58,-1.84,-2.02,-1.90,-2.07,-2.04,-2.08,  nan,-1.92,  nan,  nan,  nan,-2.03,  nan,  nan,-1.03,  nan,-2.07,-2.13,-1.76,  nan,-2.09,-1.78,-2.04,  nan,  nan,-1.14,-1.88,-1.88,  nan,  nan,  nan,  nan,-1.83,-2.05, lr 0.00092 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 49.83s | mean loss -1.83 | pos losses   nan,  nan,-0.28,-0.76,-0.91,  nan,  nan,-1.19,  nan,-1.27,-1.28,-1.21,-1.45,  nan,  nan,-1.47,  nan,  nan,  nan,-1.62,-1.52,  nan,-1.53,-1.74,  nan,-1.70,  nan,  nan,  nan,  nan,-1.75,-1.80,  nan,  nan,-1.74,-1.98,-1.88,-1.84,-1.96,-1.87,  nan,-1.84,-1.96,-2.00,  nan,-2.00,-1.87,-1.98,  nan,  nan,-2.05,  nan,-1.96,  nan,-2.04,  nan,-2.00,-1.84,  nan,-1.98,-2.04,  nan,  nan,-2.07,-2.09,-1.98,-2.04,-1.99,-2.08,  nan,-2.08,  nan,-2.00,-2.09,  nan,  nan,  nan,  nan,  nan,-2.11,-1.96,-2.03,  nan,  nan,-2.04,  nan,-2.07,-2.07,  nan,-2.02,-2.06,-2.03,-1.63,  nan,-2.14,-2.10,  nan,-1.82,-2.03,-1.99, lr 0.00096 data time  0.24 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 49.16s | mean loss -1.72 | pos losses   nan, 0.57,  nan,-0.67,  nan,-0.77,-0.75,  nan,-1.21,  nan,-1.21,-1.39,-1.31,-1.37,-1.58,-1.54,-1.57,-1.44,  nan,-1.80,-1.73,-1.44,  nan,  nan,-1.78,-1.84,-1.79,  nan,-1.86,-1.37,-1.90,-1.63,-1.94,-1.86,-1.62,-1.82,-1.98,-1.99,-2.01,  nan,-1.91,-1.95,-1.94,-1.52,-1.90,-1.95,-1.76,  nan,-1.97,-2.08,  nan,-2.06,  nan,-2.08,-1.83,-2.12,  nan,-1.96,-1.79,-1.79,-1.97,-1.97,-1.98,  nan,-1.78,-2.17,  nan,  nan,  nan,  nan,-2.07,-1.70,-1.88,-1.94,  nan,-1.84,-1.96,-2.05,  nan,-2.06,  nan,  nan,  nan,-2.02,-2.16,-1.90,-1.73,-2.03,  nan,-1.57,-1.59,-1.29,-1.67,  nan,-1.94,-1.99,  nan,  nan,  nan,  nan, lr 0.001 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 50.17s | mean loss -1.79 | pos losses   nan,  nan,-0.44,-0.76,-0.96,  nan,-1.12,-0.94,  nan,  nan,-1.37,-1.44,-1.39,-1.48,  nan,  nan,-1.58,  nan,  nan,-1.63,-1.72,-1.71,-1.64,-1.88,-1.84,-1.75,  nan,-1.85,-1.74,  nan,  nan,  nan,-1.85,-1.88,-1.97,  nan,-1.84,-1.94,  nan,-1.89,  nan,-1.81,-1.99,  nan,  nan,-2.03,-1.87,  nan,-1.84,  nan,-1.84,-2.02,  nan,-2.12,-1.84,-2.13,-2.04,-2.03,  nan,-1.82,  nan,-2.15,-1.99,-1.95,-2.17,  nan,  nan,-2.00,-1.88,  nan,-2.09,  nan,-2.03,-1.94,-1.83,  nan,  nan,-2.00,-2.09,  nan,  nan,  nan,-2.11,  nan,  nan,-1.87,-2.18,-1.94,  nan,  nan,-1.73,  nan,-2.19,-1.85,  nan,-1.92,-2.01,-2.11,-2.08,-1.35, lr 0.0009995614150494292 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 49.72s | mean loss -1.85 | pos losses   nan, 0.79,  nan,-0.85,-0.88,  nan,-1.02,-1.14,  nan,  nan,  nan,  nan,-1.47,-1.53,-1.38,  nan,-1.52,-1.67,-1.59,  nan,  nan,  nan,  nan,  nan,-1.82,  nan,-1.85,-1.87,-1.88,-1.83,-1.82,  nan,  nan,-1.92,-1.98,-1.87,  nan,  nan,-2.01,  nan,-1.99,-1.98,-1.99,-2.02,-2.08,-1.94,-2.06,  nan,-2.06,-1.99,-2.00,-1.99,-2.06,  nan,  nan,-2.04,  nan,-1.98,-1.94,-1.71,  nan,-2.12,-2.12,-2.10,  nan,-2.16,  nan,-2.08,  nan,-2.16,-2.03,-2.03,  nan,-2.05,  nan,  nan,  nan,  nan,  nan,-2.10,-2.02,-2.16,-2.07,  nan,-2.11,-1.45,-2.12,  nan,  nan,-2.09,-2.20,-2.19,-2.15,-1.92,-2.02,-2.04,-2.08,  nan,-2.11,-1.51, lr 0.0009982464296247522 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 49.94s | mean loss -1.84 | pos losses   nan, 1.50,-0.52,-0.71,  nan,  nan,-1.11,-1.13,-1.32,  nan,  nan,  nan,  nan,-1.41,-1.65,  nan,-1.51,  nan,-1.74,-1.69,  nan,-1.79,-1.81,-1.52,-1.96,-1.77,-1.90,-1.75,  nan,  nan,-1.93,  nan,-1.75,-1.90,-1.92,-2.00,-2.00,-1.62,-1.93,-2.00,  nan,-2.06,  nan,-2.01,-2.02,-1.97,  nan,-2.14,-2.08,  nan,-2.06,-2.17,-2.06,-2.14,-2.00,-2.16,  nan,  nan,-2.12,-2.17,-2.01,-2.10,-1.44,  nan,-2.08,  nan,-2.12,-1.96,-1.97,-2.07,-2.01,-2.14,-2.10,-2.20,  nan,  nan,  nan,  nan,  nan,-2.06,-1.82,-2.17,-1.61,-2.20,  nan,-2.21,  nan,  nan,-2.16,  nan,-2.05,-1.94,  nan,-1.94,  nan,-1.95,-2.17,-2.09,-1.83,-2.19, lr 0.000996057350657239 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 49.61s | mean loss -1.74 | pos losses   nan, 0.49,  nan,-0.45,  nan,  nan,-1.08,-1.20,-1.42,-1.37,  nan,-1.48,-1.28,-1.40,-1.68,  nan,-1.61,  nan,-1.48,  nan,-1.77,  nan,  nan,-1.71,-1.78,-1.62,-1.77,-1.86,-1.62,-1.82,  nan,  nan,-1.93,-1.93,-2.01,  nan,  nan,  nan,-1.97,  nan,  nan,  nan,-1.54,  nan,-1.98,-2.01,  nan,-1.95,-1.81,  nan,-1.93,-1.95,  nan,  nan,  nan,  nan,-2.13,-1.83,-1.73,  nan,  nan,  nan,-1.90,  nan,-2.04,-1.98,-2.07,-1.99,  nan,-2.04,-2.05,  nan,-2.02,-2.05,-1.71,  nan,  nan,-1.96,  nan,-1.89,-1.97,-1.73,  nan,-2.06,  nan,  nan,  nan,-2.01,-2.14,-1.77,-1.79,  nan,-2.05,-2.02,-2.03,-2.15,  nan,  nan,-2.18,-1.92, lr 0.0009929980185352525 data time  0.29 step time  0.12 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 50.02s | mean loss -1.86 | pos losses   nan,  nan, 0.02,-0.79,  nan,  nan,  nan,-1.22,  nan,-1.38,-1.38,  nan,-0.94,-1.51,-1.54,-1.51,-1.48,-1.64,-1.71,  nan,-1.71,-1.83,-1.81,  nan,-1.92,-1.82,-1.86,  nan,-1.90,  nan,-1.86,-1.94,-1.96,  nan,  nan,-1.79,-2.00,-1.94,-1.92,  nan,-1.89,  nan,  nan,-2.03,-1.96,  nan,  nan,-2.02,  nan,-2.06,  nan,-2.11,-2.02,-2.08,  nan,-2.08,-2.04,-2.02,-2.06,  nan,-1.97,  nan,  nan,  nan,  nan,  nan,-1.82,  nan,-2.05,-2.12,-2.10,-2.13,-2.13,-2.09,-2.08,  nan,  nan,-1.98,-2.14,-2.15,  nan,  nan,  nan,  nan,  nan,  nan,-1.95,-2.11,-1.70,-2.10,-1.78,  nan,-1.92,-2.01,  nan,-2.05,-1.98,-1.98,-2.17,-2.02, lr 0.0009890738003669028 data time  0.22 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 50.08s | mean loss -1.90 | pos losses   nan,  nan,-0.45,-0.87,  nan,-1.06,-1.02,  nan,-1.28,-1.37,-1.25,-1.46,  nan,-1.40,  nan,  nan,  nan,  nan,-1.69,-1.79,-1.80,-1.72,-1.87,-1.78,-1.80,  nan,  nan,-1.89,  nan,-1.87,-1.98,-1.66,-1.90,-1.97,  nan,-2.01,-2.03,  nan,-2.08,  nan,-2.03,  nan,-2.07,-1.90,-2.07,-2.07,-2.01,  nan,-2.11,-2.13,-2.08,-2.00,-2.04,-2.20,-2.05,  nan,-2.05,-2.20,-2.04,-1.84,  nan,-2.17,-2.20,-2.09,  nan,  nan,-2.23,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.23,-2.20,-2.21,-2.16,-2.09,-2.23,-1.23,  nan,  nan,-2.06,-2.02,  nan,  nan,  nan,-2.10,  nan,  nan,-2.12,  nan,-1.94,  nan,  nan,  nan,-2.12,-2.24,  nan, lr 0.0009842915805643156 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 50.08s | mean loss -1.90 | pos losses   nan,  nan,-0.52,  nan,-1.00,-0.99,-1.23,-1.18,-1.20,  nan,-1.45,-1.41,-1.47,-1.60,  nan,-1.69,  nan,-1.63,-1.72,-1.77,-1.72,-1.82,  nan,-1.85,  nan,  nan,  nan,-1.90,-1.90,-1.83,-1.98,-2.00,-1.96,-2.00,  nan,-1.99,  nan,-2.07,-2.07,-2.08,  nan,-1.99,-2.11,  nan,-1.98,-2.07,-2.08,-2.04,-2.11,-1.95,-2.13,-2.09,-2.10,  nan,  nan,-2.02,  nan,-2.11,  nan,  nan,-2.09,-2.19,  nan,-2.12,-2.20,  nan,-2.13,-2.14,  nan,  nan,  nan,-2.18,-2.11,-2.19,  nan,-2.23,-2.20,  nan,-2.05,  nan,  nan,  nan,  nan,  nan,  nan,-2.17,-2.16,  nan,-1.99,-2.22,  nan,-2.19,-2.09,-2.23,  nan,  nan,-2.12,-2.01,-2.14,  nan, lr 0.0009786597487660335 data time  0.20 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 50.11s | mean loss -1.94 | pos losses   nan, 0.04,  nan,  nan,-0.99,-0.87,  nan,  nan,-1.33,  nan,-1.36,  nan,  nan,  nan,-1.55,-1.65,-1.69,-1.66,-1.76,  nan,-1.77,-1.87,-1.83,-1.88,  nan,  nan,  nan,  nan,  nan,-1.92,  nan,-1.93,-2.01,-1.99,-1.98,  nan,-2.05,  nan,  nan,-2.11,-1.95,-2.10,-2.12,-2.00,  nan,-2.07,  nan,  nan,  nan,  nan,  nan,-2.21,-2.10,-2.06,-2.07,-2.08,-2.01,-2.16,-2.15,-1.84,-2.21,  nan,  nan,  nan,-2.06,-2.16,  nan,-2.08,-2.08,-2.22,  nan,-2.20,-2.10,  nan,  nan,-2.12,-2.12,-2.11,-2.20,-2.02,-2.12,  nan,  nan,  nan,  nan,  nan,-2.18,-1.86,-2.23,-1.88,-2.21,  nan,-2.25,-2.15,-2.19,  nan,-2.18,  nan,-2.15,  nan, lr 0.0009721881851187406 data time  0.33 step time  0.12 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 49.38s | mean loss -1.90 | pos losses   nan,  nan,  nan,-0.82,-0.96,-1.01,  nan,-1.18,-1.36,-1.42,-1.42,-1.31,-1.58,-1.56,-1.53,-1.55,  nan,-1.74,-1.48,-1.59,-1.74,  nan,  nan,-1.81,-1.84,-1.66,-1.87,-1.94,-1.81,-1.97,  nan,-1.96,  nan,-2.00,  nan,  nan,-1.98,  nan,  nan,-1.85,  nan,-2.01,-2.10,  nan,  nan,  nan,-2.06,-1.93,-2.12,-2.10,-2.12,  nan,-2.16,-2.19,-2.05,  nan,  nan,-1.94,-2.13,-2.23,-2.03,-2.20,-2.14,  nan,-2.21,-1.93,-2.15,-1.92,-2.24,-2.11,-2.15,-2.10,  nan,  nan,-2.15,-2.16,-2.05,  nan,  nan,-2.05,  nan,-2.16,  nan,-2.16,-2.14,  nan,-1.94,-2.21,  nan,-2.16,  nan,-2.09,  nan,  nan,-2.23,  nan,-1.82,  nan,-2.16,  nan, lr 0.0009648882429441257 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 50.17s | mean loss -1.87 | pos losses   nan,-0.19,-0.66, 0.00,  nan,-1.15,-1.10,  nan,  nan,-1.38,-1.42,-1.58,-1.53,-1.62,  nan,-1.68,-1.61,-1.66,-1.64,-1.79,  nan,  nan,-1.75,-1.81,-1.77,  nan,-1.85,  nan,-1.84,  nan,-1.81,  nan,-1.99,-1.87,-1.97,-1.94,-2.07,-2.02,-1.97,-2.04,-1.92,-2.01,-2.06,  nan,  nan,  nan,-1.76,  nan,-2.13,  nan,  nan,  nan,-2.14,  nan,-2.15,-2.12,  nan,-1.98,  nan,  nan,  nan,  nan,-2.09,-2.11,-2.07,-2.07,-2.16,  nan,-1.98,  nan,  nan,  nan,-2.10,-2.15,-2.10,  nan,  nan,-2.12,-1.93,  nan,-1.89,  nan,-2.08,-2.19,  nan,-2.10,-1.98,  nan,-1.91,  nan,  nan,-2.14,-2.01,-1.92,-2.18,-2.12,  nan,-1.80,-1.88,-2.16, lr 0.0009567727288213005 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 50.02s | mean loss -1.83 | pos losses   nan,-0.01,-0.32,  nan,-0.87,-1.04,-1.17,-1.18,-1.33,  nan,-1.43,-1.40,-1.49,-1.60,-1.60,  nan,  nan,-1.63,-1.77,-1.57,-1.81,-1.85,-1.78,-1.71,  nan,-1.88,-1.80,  nan,-1.92,-1.93,-1.93,  nan,-2.00,  nan,-2.02,-1.95,  nan,-2.07,  nan,  nan,  nan,  nan,  nan,  nan,-2.05,-2.03,  nan,  nan,-2.06,-2.08,  nan,  nan,-1.75,-2.20,  nan,-2.05,  nan,-1.99,  nan,  nan,-2.14,-2.20,  nan,-1.98,-1.95,  nan,-2.18,-2.08,  nan,-1.61,-1.72,-2.15,  nan,  nan,-1.88,  nan,  nan,  nan,-2.19,-1.92,  nan,  nan,  nan,-2.23,-2.18,-2.17,  nan,-2.30,  nan,  nan,  nan,  nan,-2.16,  nan,  nan,  nan,-2.17,-2.08,-2.07,-1.96, lr 0.0009478558801197064 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 49.98s | mean loss -1.84 | pos losses   nan, 0.34,-0.73,-0.81,  nan,-1.03,-1.16,-1.24,-1.32,  nan,-1.47,-1.49,-1.54,  nan,-1.62,  nan,-1.67,-1.71,  nan,  nan,  nan,-1.81,-1.71,  nan,  nan,  nan,  nan,-1.91,-1.95,  nan,  nan,-1.99,-1.88,  nan,-2.00,  nan,  nan,-2.05,-2.01,-2.08,-1.98,-1.88,  nan,-2.04,  nan,  nan,  nan,-2.09,-2.05,-2.03,-2.10,-2.19,  nan,-2.15,  nan,-2.10,-2.20,  nan,  nan,  nan,-2.18,  nan,-2.23,  nan,-2.14,  nan,  nan,-2.19,  nan,-2.06,-2.04,  nan,-2.17,-2.17,-2.15,  nan,  nan,-2.25,-2.16,-2.08,-1.91,-2.08,  nan,  nan,  nan,  nan,-2.15,-2.14,-2.20,-1.76,  nan,  nan,  nan,  nan,-1.89,-1.95,  nan,-2.20,-2.11,-1.63, lr 0.0009381533400219318 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 49.75s | mean loss -1.93 | pos losses   nan, 0.81,-0.67,-0.89,  nan,  nan,-1.09,-1.10,-1.29,-1.39,-1.44,-1.48,  nan,-1.53,  nan,  nan,  nan,-1.70,-1.78,-1.76,-1.83,  nan,-1.82,-1.89,  nan,  nan,  nan,-1.98,-1.95,  nan,-2.02,-1.81,-2.02,-2.06,-2.05,-1.84,-1.97,  nan,  nan,-2.12,-1.99,  nan,-2.01,  nan,  nan,  nan,-1.99,  nan,  nan,-1.98,  nan,-2.16,  nan,-2.17,-2.13,-2.17,  nan,-2.08,-2.17,  nan,-2.03,-2.25,-2.25,-2.19,-2.20,-2.05,-1.94,-2.00,-2.22,-2.10,-2.17,  nan,-2.06,-2.02,-2.20,  nan,  nan,  nan,-2.23,-2.15,-2.27,  nan,-2.17,-2.18,  nan,  nan,-2.20,-2.14,-2.02,  nan,  nan,-2.23,-1.98,  nan,-2.14,-2.08,  nan,-1.85,-2.19,  nan, lr 0.0009276821300802534 data time  0.31 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 49.79s | mean loss -1.94 | pos losses   nan,  nan,  nan,  nan,  nan,  nan,-1.13,-1.19,-1.33,-1.31,-1.38,-1.50,-1.58,-1.61,-1.66,  nan,  nan,  nan,  nan,-1.83,-1.78,-1.73,  nan,  nan,-1.81,  nan,  nan,-1.78,-1.90,-1.82,  nan,-2.05,-1.95,-2.12,-1.87,  nan,  nan,-1.98,  nan,-2.08,-2.06,  nan,-2.14,-2.11,-1.76,  nan,-1.99,-2.13,  nan,-2.14,-2.07,-2.14,  nan,-2.13,  nan,-2.11,-2.19,-2.15,  nan,-1.96,-2.02,-2.09,-2.10,-2.25,-2.24,-2.15,-2.18,  nan,  nan,  nan,  nan,-2.01,-2.22,  nan,-2.16,-2.13,-2.15,-1.90,-2.09,  nan,  nan,-2.23,  nan,-2.25,-2.25,-2.14,-2.14,  nan,-2.14,-1.73,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.18,-2.23,-2.16, lr 0.0009164606203550497 data time  0.20 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 49.92s | mean loss -1.96 | pos losses   nan,  nan,  nan,-0.83,-1.01,-1.03,-1.13,-1.17,-1.33,-1.44,-1.39,  nan,  nan,-1.71,-1.68,-1.64,  nan,-1.65,-1.65,-1.80,  nan,  nan,-1.82,  nan,-1.72,-1.93,  nan,-1.76,-2.00,  nan,-2.05,-1.84,  nan,-2.03,-2.10,  nan,  nan,  nan,  nan,  nan,-2.16,-2.02,  nan,-2.15,-2.20,-2.14,  nan,-2.12,-2.12,-2.19,  nan,-2.10,-2.31,-1.99,-2.17,-2.02,-2.13,-2.01,-2.19,  nan,-2.16,-2.13,-2.13,-2.32,-2.12,-2.14,-2.18,-2.20,-1.88,  nan,-2.20,  nan,-2.09,  nan,-2.12,-2.22,-2.10,-1.42,  nan,-2.38,  nan,-2.24,-2.20,-2.03,  nan,  nan,  nan,-1.86,  nan,-2.23,-1.95,-2.22,  nan,  nan,-2.21,-2.05,  nan,-2.18,  nan,  nan, lr 0.0009045084971874737 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 50.05s | mean loss -1.89 | pos losses   nan, 0.17,  nan,-0.95,-0.99,-1.11,-1.15,-1.19,-1.29,  nan,  nan,-1.41,-1.60,-1.64,  nan,  nan,-1.71,-1.79,-1.78,  nan,  nan,-1.84,-1.83,-1.93,  nan,  nan,  nan,-1.98,  nan,-1.95,  nan,-1.96,-1.85,-1.92,-1.89,-2.10,-2.06,-2.06,-2.13,  nan,  nan,-2.22,-2.09,-1.96,-2.15,-1.91,  nan,-2.06,  nan,-2.19,-2.18,  nan,  nan,-2.17,  nan,-2.20,-2.20,-2.18,-2.11,-2.13,  nan,  nan,-2.02,-2.29,-2.17,  nan,  nan,-2.20,-2.09,-2.18,  nan,-1.98,-2.26,-2.15,-2.07,  nan,  nan,  nan,-2.28,-2.27,  nan,-2.06,  nan,  nan,  nan,-2.17,-2.20,-1.98,-2.27,-2.24,-2.19,-2.29,-2.26,-2.33,-2.23,-2.03,  nan,-2.12,-2.08,  nan, lr 0.0008918467286629199 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 50.03s | mean loss -2.03 | pos losses   nan,  nan,-0.70,-0.69,-1.03,-1.15,-1.14,-1.26,  nan,  nan,  nan,-1.51,-1.61,-1.65,-1.63,-1.57,  nan,-1.73,  nan,  nan,-1.87,-1.85,  nan,  nan,  nan,  nan,-1.96,-1.97,  nan,  nan,-1.94,-2.07,-2.06,-2.09,  nan,-2.16,-2.22,-2.11,  nan,  nan,  nan,  nan,-2.12,  nan,  nan,-2.09,  nan,-2.14,-2.20,  nan,  nan,  nan,  nan,-2.25,  nan,-2.25,-2.30,-2.23,-2.21,-2.23,-2.28,-2.24,-2.23,  nan,-2.25,  nan,  nan,-2.21,-2.24,  nan,-2.12,-2.27,  nan,-2.20,-2.25,  nan,-2.25,  nan,-2.19,-2.26,  nan,-2.27,-2.33,  nan,  nan,-2.23,-2.26,  nan,-2.25,-2.21,-2.28,-2.37,-2.17,  nan,-2.22,  nan,-2.15,  nan,-2.24,-2.28, lr 0.0008784975278258782 data time  0.27 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 49.22s | mean loss -1.92 | pos losses   nan, 0.04,-0.56,-0.94,-0.77,-1.08,-1.24,-1.30,-1.37,-1.41,-1.49,-1.36,-1.41,  nan,-1.60,-1.68,  nan,-1.70,  nan,-1.82,-1.85,  nan,  nan,  nan,-1.85,  nan,-2.00,-1.96,-1.92,-2.02,  nan,-2.07,  nan,-1.96,  nan,-2.05,-2.07,-2.10,  nan,-2.13,-2.09,-2.16,  nan,  nan,-2.16,-2.15,  nan,-2.20,-2.24,-2.07,  nan,  nan,  nan,  nan,-2.20,  nan,-2.14,-2.10,-2.24,-2.29,-2.16,-2.04,-2.21,-2.20,-2.29,  nan,  nan,  nan,-2.22,  nan,-2.20,  nan,  nan,  nan,  nan,-2.33,  nan,  nan,-2.12,-2.25,  nan,-2.27,-2.22,-1.90,-2.21,-2.28,-2.28,-2.27,-2.24,  nan,-2.07,  nan,  nan,  nan,  nan,-2.23,-1.97,  nan,-2.15,-2.20, lr 0.0008644843137107057 data time  0.16 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 49.68s | mean loss -2.03 | pos losses   nan,  nan,-0.70,-0.84,-0.96,-1.12,-1.21,-1.34,-1.36,  nan,-1.44,-1.57,  nan,-1.56,  nan,  nan,-1.74,  nan,  nan,  nan,  nan,-1.89,-1.86,  nan,-1.94,-2.01,-1.97,-1.98,  nan,-2.00,-1.98,-2.04,-2.03,  nan,-2.01,  nan,-2.03,  nan,-2.14,  nan,  nan,-2.12,-2.17,-2.11,-2.06,-2.19,-2.13,-2.15,  nan,-2.24,  nan,-2.24,-2.20,  nan,  nan,  nan,-2.33,  nan,-2.33,  nan,-2.25,  nan,-2.18,  nan,-2.23,  nan,  nan,-2.21,-2.21,-2.28,-2.26,  nan,  nan,-2.27,  nan,-2.31,-2.11,-2.26,-2.18,-2.20,-2.30,  nan,  nan,-2.26,-2.26,-2.27,-2.19,-2.26,  nan,  nan,-2.20,-2.28,-2.28,-2.27,-2.21,-2.34,-2.32,-2.27,  nan,-2.23, lr 0.0008498316702566827 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 49.65s | mean loss -2.04 | pos losses   nan, 0.03,  nan,-0.95,-1.03,  nan,  nan,  nan,  nan,  nan,-1.51,  nan,  nan,  nan,-1.66,  nan,-1.72,-1.76,-1.75,-1.84,-1.89,-1.79,-1.91,-1.87,  nan,-1.92,-1.91,-1.98,-1.95,  nan,-2.06,-2.06,-2.07,-2.03,  nan,-2.05,  nan,  nan,-2.08,-2.17,-2.17,  nan,-2.12,  nan,  nan,-2.16,-2.16,  nan,-2.17,  nan,-2.16,-2.27,  nan,-2.23,-2.16,-2.25,  nan,-2.20,  nan,-2.23,-2.28,  nan,-2.27,-2.24,-2.23,-2.27,-2.23,-2.26,  nan,-2.20,  nan,-2.13,-2.29,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.22,-2.29,-2.25,  nan,-2.31,-2.19,-2.01,-2.26,  nan,-2.23,-2.23,-2.20,-2.14,-2.32,  nan,-2.28,-2.24,  nan,-2.11,  nan, lr 0.0008345653031794292 data time  0.20 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 49.98s | mean loss -1.99 | pos losses   nan, 0.09,-0.69,  nan,  nan,  nan,-1.21,  nan,-1.40,  nan,-1.45,  nan,-1.58,-1.58,-1.54,-1.65,-1.70,-1.77,-1.76,  nan,-1.89,-1.90,-1.87,-1.92,-1.93,-2.03,-1.90,-2.11,-1.98,  nan,  nan,-2.04,-2.05,  nan,-2.13,-2.03,  nan,  nan,-2.14,-2.23,  nan,-2.22,  nan,-2.12,-2.15,  nan,-2.09,-2.20,-2.13,  nan,-2.19,-2.20,-2.09,-2.21,  nan,-2.15,-2.17,-2.23,  nan,-2.25,-2.06,-1.90,-2.08,-2.08,-2.21,  nan,  nan,-2.19,  nan,-2.18,-2.20,-2.27,-2.25,-2.13,-2.34,-1.92,-2.27,-2.08,  nan,  nan,-2.30,-2.31,-2.21,  nan,-2.21,  nan,  nan,  nan,-2.26,  nan,-2.14,  nan,  nan,  nan,-2.22,  nan,-2.24,  nan,  nan,-2.18, lr 0.0008187119948743449 data time  0.20 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 49.66s | mean loss -2.02 | pos losses   nan,-0.29,-0.65,-0.78,  nan,-1.20,-1.18,-1.17,-1.22,-1.47,  nan,-1.47,-1.54,  nan,  nan,-1.73,-1.74,-1.77,  nan,  nan,-1.84,-1.84,-1.82,  nan,  nan,-1.94,-1.99,-1.99,  nan,-2.05,-2.05,-2.05,-2.08,  nan,-2.13,  nan,  nan,  nan,-2.16,-2.16,-2.06,  nan,-2.20,-2.18,-2.14,  nan,-2.18,-2.12,-2.07,-2.19,  nan,-2.22,  nan,-2.20,-2.20,-2.21,-2.30,-2.13,-2.11,  nan,  nan,-2.18,  nan,  nan,-2.28,-2.26,  nan,  nan,-2.30,-2.14,-2.21,-2.30,-2.25,  nan,-2.18,  nan,  nan,  nan,-2.20,-2.30,-2.17,  nan,-2.29,  nan,-2.14,-2.30,-2.32,-2.30,-2.34,-2.33,  nan,-2.28,-2.24,-2.30,-2.29,-1.95,  nan,  nan,-2.27,-2.16, lr 0.0008022995574311875 data time  0.29 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 49.77s | mean loss -1.91 | pos losses   nan, 0.46,  nan,-0.87,-0.98,-1.18,  nan,-1.28,-1.15,-1.44,  nan,-1.51,  nan,-1.57,-1.51,-1.72,  nan,-1.73,-1.62,  nan,  nan,-1.68,  nan,  nan,-1.96,-1.77,  nan,  nan,  nan,-1.97,  nan,  nan,  nan,  nan,-1.97,-2.08,-1.92,  nan,  nan,-2.10,-2.14,-1.96,  nan,-2.21,  nan,-2.19,  nan,-2.26,-2.06,  nan,  nan,  nan,-2.23,-2.22,-2.22,-2.14,-2.22,-2.15,-2.12,  nan,-2.04,-2.07,-1.98,  nan,  nan,-2.20,  nan,-2.28,  nan,-2.22,  nan,-2.29,-2.24,  nan,-2.22,  nan,  nan,  nan,-2.10,-2.26,-2.29,-2.16,-2.17,-2.25,  nan,-2.21,-2.22,  nan,-2.11,-2.11,-2.29,  nan,  nan,  nan,  nan,-2.05,-2.04,-2.19,-2.24,-2.07, lr 0.000785356783842216 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 49.99s | mean loss -2.06 | pos losses   nan, 0.05,-0.63,  nan,-1.07,  nan,-1.14,-1.34,-1.45,-1.44,  nan,-1.60,-1.65,  nan,-1.68,  nan,-1.73,  nan,-1.81,  nan,-1.89,-1.96,  nan,  nan,-1.94,  nan,  nan,-2.09,  nan,-2.08,-2.07,-2.09,  nan,  nan,  nan,-2.12,-2.04,-2.17,-2.10,-2.17,-2.12,-2.17,-2.18,-2.08,  nan,  nan,  nan,  nan,-2.23,-2.21,-2.24,-2.24,-2.18,-2.25,-2.25,  nan,-2.25,-2.26,-2.26,-2.28,  nan,-2.26,-2.34,-2.32,-2.29,  nan,-2.23,  nan,-2.33,-2.26,-2.31,  nan,-2.29,-2.26,  nan,-2.33,  nan,-2.14,-2.29,  nan,  nan,-2.34,-2.31,-2.10,-2.31,-2.26,  nan,-2.07,-2.28,-2.27,  nan,-2.10,-2.09,-2.36,  nan,-2.26,-2.35,-2.21,-2.23,  nan, lr 0.0007679133974894983 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 50.10s | mean loss -2.03 | pos losses   nan, 0.02,-0.77,-0.86,  nan,  nan,-1.20,-1.27,  nan,-1.53,  nan,-1.38,-1.61,-1.65,-1.67,-1.61,  nan,-1.77,-1.82,-1.85,  nan,-1.83,-1.96,  nan,-1.85,-1.96,  nan,-1.95,-2.02,  nan,  nan,-2.06,-2.05,-2.13,-2.11,  nan,-2.14,  nan,-2.16,  nan,  nan,  nan,-2.22,-2.06,-2.20,-2.20,-2.21,  nan,-2.26,  nan,-2.05,-2.29,-2.27,  nan,-2.21,-2.26,-2.29,-2.27,  nan,  nan,  nan,-2.28,-2.26,-2.29,  nan,  nan,-2.24,-2.35,-2.22,-2.28,-2.30,-2.25,  nan,-2.34,-2.27,-2.28,  nan,-2.21,  nan,-2.31,  nan,-2.30,  nan,-2.36,-2.30,  nan,  nan,-2.27,-2.26,-2.16,-2.25,  nan,-2.30,  nan,  nan,-2.32,  nan,  nan,-2.36,-2.28, lr 0.00075 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 49.92s | mean loss -1.97 | pos losses   nan, 0.14,-0.58,-0.89,-1.05,  nan,  nan,-1.33,-1.37,-1.42,  nan,-1.50,-1.61,-1.65,-1.51,  nan,  nan,-1.71,-1.78,-1.80,-1.86,  nan,  nan,  nan,  nan,-1.94,-1.96,-1.98,  nan,  nan,-2.12,-2.07,-2.13,-2.15,-1.96,-2.07,  nan,-2.14,  nan,-2.15,-2.20,  nan,-2.22,-2.17,  nan,-2.19,-2.19,-2.21,-2.15,-2.08,-2.21,-2.19,-2.17,-2.19,-2.22,-2.24,-2.28,-2.02,-2.19,-2.21,  nan,-2.19,  nan,-2.25,  nan,-2.28,-2.16,  nan,-2.23,  nan,-2.23,  nan,  nan,-2.22,  nan,  nan,  nan,-2.13,  nan,  nan,  nan,-2.15,  nan,  nan,  nan,-2.26,-2.31,-2.16,-2.27,  nan,-2.32,-2.37,  nan,-1.98,-2.27,-2.32,-1.51,-2.18,-2.27,  nan, lr 0.0007316480175599309 data time  0.31 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 49.33s | mean loss -2.08 | pos losses   nan, 0.03,  nan,  nan,  nan,  nan,  nan,-1.30,-1.34,-1.48,  nan,-1.60,-1.61,-1.69,-1.73,-1.80,  nan,-1.65,  nan,-1.90,-1.83,-1.99,-1.95,  nan,-1.99,-2.02,-2.03,-2.00,-2.07,-2.07,  nan,-2.18,-2.10,-2.15,  nan,  nan,-2.13,-2.14,  nan,-2.17,-2.08,-2.15,  nan,-2.19,-2.29,-2.24,-2.20,-2.24,-2.21,-2.24,-2.29,-2.24,  nan,-2.24,-2.26,-2.27,  nan,-2.31,  nan,  nan,-2.26,-2.11,  nan,-2.30,  nan,-2.21,-2.25,-2.27,  nan,-2.31,-2.23,  nan,  nan,  nan,-2.30,-2.24,-2.34,  nan,-2.31,  nan,  nan,-2.32,-2.28,-2.35,  nan,-2.31,-2.34,  nan,-2.29,-2.31,  nan,  nan,-2.36,  nan,  nan,-2.34,-2.35,-2.24,-2.37,-2.03, lr 0.0007128896457825364 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 50.30s | mean loss -2.03 | pos losses   nan,-0.34,  nan,  nan,-1.03,  nan,-1.26,-1.34,  nan,-1.42,-1.52,-1.54,  nan,-1.69,-1.50,  nan,-1.76,-1.87,-1.68,  nan,-1.86,  nan,-1.89,  nan,  nan,-1.93,-2.06,-2.00,-2.00,  nan,-2.09,  nan,-2.14,-2.13,  nan,-2.09,-2.03,-2.16,-2.14,-2.15,-2.19,  nan,-2.21,  nan,  nan,-2.19,-2.25,-2.18,-2.27,-2.22,-2.21,-2.24,  nan,  nan,  nan,  nan,-2.32,-2.30,  nan,  nan,-2.28,-2.27,-2.18,-2.25,  nan,-2.28,-2.33,-2.17,-2.27,-2.23,-2.28,-2.32,-2.35,-2.23,-2.32,-2.22,-2.33,  nan,-2.30,  nan,-2.16,-2.37,-2.33,  nan,  nan,-2.31,-2.34,-2.29,  nan,  nan,-2.30,-2.23,-2.34,-2.33,-2.30,  nan,-2.35,  nan,  nan,-2.36, lr 0.0006937577932260515 data time  0.22 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 50.45s | mean loss -2.02 | pos losses   nan,  nan,-0.64,-0.79,-0.99,  nan,-1.18,  nan,-1.33,-1.40,  nan,-1.58,-1.61,-1.70,  nan,-1.73,-1.75,-1.82,  nan,-1.78,-1.85,-1.88,-1.93,-2.00,-1.95,-2.09,  nan,-2.04,-2.06,-2.02,-2.08,  nan,  nan,  nan,  nan,-2.10,-2.14,-2.07,  nan,-2.27,-2.18,-2.19,-2.18,-2.20,  nan,  nan,-2.19,  nan,-2.23,-2.21,-2.27,-2.23,-2.22,-2.23,-2.23,-2.26,-2.15,-2.26,-2.25,-2.31,-2.28,-2.28,  nan,-2.29,-2.35,-2.29,  nan,  nan,  nan,  nan,-2.27,  nan,  nan,  nan,  nan,  nan,-2.35,-2.27,  nan,-2.19,  nan,-2.29,-2.32,-2.31,-2.34,-2.41,  nan,-2.10,  nan,  nan,  nan,  nan,  nan,-2.31,-2.32,-2.31,  nan,  nan,-2.41,-2.27, lr 0.0006742860236609076 data time  0.16 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 50.51s | mean loss -2.08 | pos losses   nan,  nan,-0.75,  nan,-1.08,  nan,-1.23,-1.34,  nan,-1.52,  nan,-1.54,-1.65,-1.68,  nan,  nan,  nan,-1.82,-1.81,-1.81,  nan,-1.88,-2.00,  nan,-1.91,-2.03,  nan,-2.02,-1.87,-2.07,-2.02,-2.10,  nan,-2.08,-2.13,-2.13,-2.14,  nan,  nan,-2.19,-2.19,-2.21,  nan,-2.21,-2.23,-2.16,-2.26,-2.19,  nan,  nan,-2.34,-2.33,  nan,-2.23,-2.17,-2.28,-2.16,-2.21,  nan,  nan,-2.30,  nan,-2.38,-2.30,-2.30,  nan,-2.31,-2.18,-2.27,-2.32,-2.27,-2.27,  nan,  nan,-2.36,-2.21,  nan,-2.34,-2.28,  nan,-2.29,-2.32,-2.22,  nan,  nan,  nan,-2.15,  nan,  nan,-2.27,  nan,  nan,-2.40,  nan,-2.27,  nan,-2.29,-2.36,-2.31,  nan, lr 0.0006545084971874737 data time  0.17 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 50.30s | mean loss -2.09 | pos losses   nan, 0.00,-0.74,-0.89,-1.07,  nan,  nan,-1.32,-1.36,  nan,-1.55,  nan,-1.64,  nan,-1.72,  nan,-1.87,-1.79,  nan,  nan,-1.91,-1.85,-1.99,  nan,  nan,  nan,-1.99,-2.09,-2.08,  nan,  nan,-2.11,-2.13,-2.13,-2.13,-2.12,-2.15,  nan,  nan,-2.17,-2.18,  nan,  nan,-2.29,-2.25,-2.28,-2.30,  nan,  nan,-2.25,-2.27,-2.23,-2.23,-2.26,-2.27,-2.25,  nan,-2.27,-2.23,-2.31,  nan,-2.27,  nan,-2.30,-2.38,-2.32,  nan,-2.32,-2.33,  nan,-2.38,  nan,-2.34,  nan,-2.35,  nan,-2.34,-2.39,-2.34,-2.28,-2.36,  nan,-2.33,  nan,-2.35,  nan,-2.22,-2.38,-2.33,-2.37,-2.30,  nan,-2.32,-2.36,-2.33,  nan,  nan,  nan,-2.39,  nan, lr 0.0006344599103076329 data time  0.29 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 49.75s | mean loss -2.11 | pos losses   nan,-0.05,-0.78,  nan,  nan,  nan,  nan,  nan,-1.43,-1.48,  nan,-1.55,-1.54,  nan,-1.64,-1.74,-1.74,  nan,  nan,-1.90,-1.88,  nan,  nan,-1.98,  nan,-2.03,-1.96,-2.03,-2.10,-2.06,-2.08,-2.06,-2.13,-2.18,  nan,-2.23,-2.17,-2.14,-2.16,  nan,  nan,  nan,  nan,-2.19,-2.23,-2.23,-2.19,  nan,-2.24,-2.24,  nan,  nan,-2.28,-2.26,-2.31,  nan,  nan,-2.25,  nan,-2.29,-2.26,-2.30,-2.28,  nan,  nan,-2.31,-2.21,-2.25,-2.27,-2.33,-2.30,-2.14,-2.35,-2.31,  nan,-2.26,  nan,-2.35,  nan,-2.29,  nan,  nan,-2.31,-2.35,  nan,-2.32,-1.97,-2.30,-2.35,-2.36,-2.30,-2.39,-2.33,  nan,-2.31,-2.37,-2.40,  nan,  nan,-2.31, lr 0.0006141754350553279 data time  0.20 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 50.73s | mean loss -2.08 | pos losses   nan,  nan,-0.75,-0.88,  nan,-1.17,  nan,-1.39,-1.40,-1.44,-1.56,-1.56,-1.59,-1.72,-1.65,-1.73,-1.75,  nan,  nan,-1.87,  nan,-1.91,-1.92,-1.94,-2.02,  nan,-2.02,-2.06,-2.05,-2.06,  nan,  nan,-2.09,-2.20,-2.18,-2.13,-2.06,  nan,  nan,  nan,-2.21,  nan,  nan,-2.30,-2.10,-2.19,-2.24,  nan,  nan,  nan,  nan,-2.35,  nan,  nan,-2.26,-2.31,-2.34,-2.31,  nan,-2.28,-2.28,  nan,-2.30,-2.31,-2.27,-2.35,-2.26,-2.30,-2.28,-2.28,-2.37,-2.18,-2.37,  nan,-2.35,-2.34,-2.34,  nan,-2.36,-2.38,-2.39,-2.34,-2.31,  nan,  nan,-2.39,  nan,-2.31,-2.32,-2.28,  nan,  nan,  nan,-2.20,-2.32,-2.31,-2.28,  nan,-2.33,-2.42, lr 0.0005936906572928624 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 50.65s | mean loss -2.09 | pos losses   nan,  nan,  nan,  nan,  nan,-1.18,-1.30,-1.34,-1.41,-1.46,-1.56,-1.58,-1.64,-1.73,  nan,  nan,-1.77,-1.82,  nan,-1.89,  nan,-1.97,  nan,  nan,-1.98,-2.06,-2.04,-2.04,  nan,-2.08,-2.07,-2.09,-2.17,-2.14,-2.19,  nan,-2.13,-2.18,  nan,  nan,-2.20,-2.24,  nan,  nan,-2.26,  nan,  nan,-2.25,  nan,  nan,-2.23,-2.25,  nan,-2.27,-2.24,-2.34,-2.28,  nan,  nan,  nan,  nan,-2.30,  nan,  nan,-2.35,  nan,-2.29,-2.35,-2.43,-2.31,-2.16,  nan,-2.34,  nan,-2.36,  nan,-2.29,-2.41,-2.36,  nan,-2.35,  nan,-2.32,-2.38,  nan,  nan,-2.34,-2.31,-2.31,  nan,-2.40,-2.26,  nan,-2.32,-2.36,-2.38,-2.37,  nan,  nan,  nan, lr 0.0005730415142812059 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 50.62s | mean loss -2.12 | pos losses   nan,-0.20,  nan,-0.95,  nan,-1.16,  nan,  nan,-1.46,-1.42,  nan,-1.57,  nan,  nan,-1.77,-1.84,-1.77,-1.80,-1.86,-1.86,  nan,-1.98,  nan,-1.92,  nan,-2.02,-2.09,  nan,-2.10,-2.08,-2.10,-2.15,-2.15,-2.11,  nan,-2.17,-2.20,-2.20,-2.22,-2.18,  nan,-2.25,-2.25,  nan,  nan,-2.27,  nan,-2.26,  nan,  nan,-2.24,-2.27,-2.19,  nan,-2.26,-2.33,-2.32,  nan,-2.35,  nan,-2.35,  nan,-2.28,-2.30,-2.41,  nan,-2.29,-2.33,  nan,-2.31,  nan,  nan,  nan,-2.36,  nan,-2.37,-2.33,  nan,-2.27,  nan,  nan,-2.34,-2.27,-2.34,-2.30,  nan,-2.36,  nan,-2.45,  nan,-2.34,-2.34,  nan,-2.26,-2.38,-2.34,-2.33,  nan,  nan,-2.43, lr 0.0005522642316338268 data time  0.21 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 50.16s | mean loss -2.11 | pos losses   nan,-0.20,  nan,-0.97,-0.98,-1.14,-1.31,-1.39,-1.42,-1.54,  nan,-1.62,-1.66,-1.62,-1.77,  nan,-1.80,-1.83,-1.93,  nan,  nan,-1.96,  nan,  nan,  nan,-2.00,-2.08,-2.08,-2.08,-2.04,  nan,-2.11,-2.16,  nan,  nan,  nan,  nan,-2.17,-2.19,-2.13,-2.26,  nan,  nan,-2.26,-2.19,-2.26,-2.24,-2.24,-2.35,-2.27,-2.42,  nan,  nan,  nan,-2.33,-2.27,-2.22,  nan,-2.40,-2.30,-2.32,  nan,-2.37,-2.30,  nan,-2.33,  nan,  nan,  nan,-2.31,-2.33,  nan,-2.37,-2.43,-2.37,-2.36,-2.35,  nan,  nan,  nan,-2.35,-2.33,-2.29,-2.37,  nan,  nan,-2.32,  nan,-2.32,-2.39,-2.30,-2.32,  nan,  nan,-2.36,-2.34,-2.38,-2.31,-2.23,-2.38, lr 0.0005313952597646568 data time  0.30 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 49.40s | mean loss -2.15 | pos losses   nan,  nan,  nan,  nan,-1.10,-1.15,  nan,-1.33,-1.43,  nan,  nan,-1.66,  nan,-1.72,-1.73,-1.76,  nan,-1.89,-1.91,-1.94,-1.86,  nan,-1.99,  nan,  nan,-2.03,-2.07,-2.00,-2.15,-2.10,-2.14,-2.08,-2.16,  nan,  nan,  nan,  nan,  nan,-2.24,-2.21,-2.13,  nan,  nan,-2.19,-2.31,  nan,-2.26,-2.22,  nan,-2.19,-2.26,-2.22,-2.29,-2.29,-2.33,  nan,-2.34,-2.36,  nan,-2.32,  nan,  nan,-2.39,-2.34,  nan,  nan,-2.33,-2.38,-2.43,  nan,-2.36,  nan,-2.41,-2.44,  nan,-2.26,-2.39,-2.42,-2.35,  nan,  nan,-2.27,  nan,  nan,  nan,-2.43,-2.36,-2.39,-2.29,  nan,-2.24,-2.37,  nan,-2.37,-2.36,-2.34,-2.44,-2.28,-2.38,-2.42, lr 0.0005104712099416785 data time  0.15 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 49.95s | mean loss -2.13 | pos losses   nan,  nan,-0.78,-0.99,-1.09,  nan,  nan,-1.37,-1.41,-1.45,-1.55,-1.59,  nan,  nan,-1.78,-1.67,  nan,-1.83,-1.98,-1.85,  nan,-2.01,  nan,-2.00,  nan,-2.04,-2.08,-2.04,-2.07,  nan,-2.15,-2.22,  nan,-2.21,-2.18,-2.13,  nan,-2.23,-2.23,-2.24,-2.22,-2.22,-2.22,  nan,-2.17,  nan,  nan,  nan,  nan,-2.31,  nan,  nan,  nan,  nan,-2.32,-2.31,-2.34,-2.35,  nan,  nan,  nan,  nan,-2.36,  nan,-2.34,  nan,-2.27,  nan,-2.35,-2.37,-2.39,  nan,-2.37,-2.35,  nan,-2.39,-2.40,-2.38,  nan,  nan,  nan,  nan,-2.40,  nan,-2.36,  nan,-2.39,  nan,-2.41,  nan,-2.41,  nan,-2.38,-2.39,-2.37,-2.36,-2.35,  nan,  nan,-2.41, lr 0.0004895287900583215 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 52.30s | mean loss -2.09 | pos losses   nan,-0.33,  nan,-0.98,  nan,-1.02,-1.14,-1.33,-1.50,  nan,-1.45,-1.57,-1.60,  nan,  nan,-1.74,  nan,-1.86,-1.92,-1.93,-1.91,-1.96,-1.95,-2.03,  nan,  nan,-2.09,-2.06,-2.10,-2.16,-2.15,-2.19,  nan,  nan,-2.11,-2.13,-2.12,-2.24,  nan,-2.26,-2.26,  nan,-2.24,  nan,-2.22,-2.30,  nan,  nan,  nan,  nan,-2.24,-2.32,  nan,-2.30,-2.33,  nan,-2.38,-2.26,  nan,-2.35,-2.35,-2.36,-2.29,  nan,-2.33,  nan,  nan,  nan,-2.33,-2.39,  nan,  nan,-2.21,  nan,-2.28,  nan,  nan,-2.36,  nan,-2.39,  nan,-2.26,-2.43,  nan,-2.38,-2.34,-2.35,  nan,-2.36,-2.39,-2.29,  nan,-2.23,-2.29,  nan,  nan,-2.39,-2.30,  nan,-2.40, lr 0.0004686047402353433 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 50.06s | mean loss -2.09 | pos losses   nan,-0.48,-0.75,  nan,  nan,  nan,-1.28,-1.39,-1.42,-1.49,-1.52,-1.71,-1.68,  nan,-1.74,  nan,-1.83,-1.83,-1.86,-1.91,-1.97,-1.92,-2.00,  nan,-2.09,-2.06,  nan,  nan,-2.13,-2.07,-2.13,  nan,-2.20,-2.16,-2.13,-2.16,-2.19,-2.20,-2.26,-2.23,  nan,  nan,  nan,-2.30,-2.19,-2.22,-2.29,-2.20,-2.26,  nan,-2.28,  nan,-2.32,-2.30,  nan,  nan,-2.33,-2.33,-2.33,  nan,  nan,-2.32,  nan,-2.32,-2.39,  nan,-2.34,  nan,-2.35,  nan,-2.33,  nan,-2.42,-2.25,-2.33,-2.36,  nan,-2.34,  nan,-2.33,-2.41,-2.36,-2.44,  nan,-2.40,  nan,-2.41,  nan,-2.36,-2.32,-2.27,  nan,-2.33,-2.38,  nan,-2.41,-2.41,  nan,-2.30,-2.29, lr 0.00044773576836617336 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 49.43s | mean loss -2.17 | pos losses   nan,  nan,-0.78,-0.96,  nan,-1.17,-1.26,-1.37,  nan,  nan,  nan,-1.64,-1.72,-1.70,-1.77,  nan,  nan,  nan,  nan,-1.93,  nan,-1.98,  nan,  nan,-2.04,-2.01,  nan,-2.08,  nan,-2.11,-2.10,  nan,-2.20,-2.16,  nan,-2.16,-2.22,-2.22,  nan,  nan,-2.23,-2.27,-2.28,  nan,-2.30,-2.31,  nan,-2.32,  nan,-2.32,  nan,-2.32,  nan,-2.35,  nan,  nan,  nan,-2.35,-2.34,  nan,-2.41,-2.42,-2.36,  nan,-2.41,-2.26,  nan,-2.30,  nan,-2.42,  nan,  nan,  nan,-2.37,  nan,  nan,-2.42,-2.33,  nan,-2.37,-2.36,  nan,-2.39,-2.44,-2.40,  nan,-2.42,  nan,-2.32,-2.43,-2.35,  nan,-2.40,-2.45,  nan,-2.32,  nan,-2.39,-2.37,  nan, lr 0.00042695848571879425 data time  0.33 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 49.96s | mean loss -2.03 | pos losses   nan,-0.38,-0.79,-0.95,-1.07,-1.20,-1.35,-1.32,-1.46,-1.53,-1.52,-1.62,  nan,-1.68,  nan,-1.86,  nan,-1.81,-1.87,  nan,-1.96,-1.97,-2.00,-1.95,-2.09,-2.07,-2.12,  nan,-2.09,-2.10,  nan,-2.13,  nan,-2.18,-2.18,  nan,-2.24,-2.20,-2.17,-2.24,  nan,  nan,  nan,  nan,-2.25,-2.35,  nan,-2.17,  nan,  nan,  nan,  nan,-2.32,-2.29,  nan,-2.35,-2.35,-2.31,-2.35,-2.44,-2.37,-2.37,-2.37,  nan,  nan,  nan,  nan,-2.34,-2.34,  nan,  nan,-2.35,-2.40,  nan,  nan,  nan,-2.41,  nan,-2.39,  nan,-2.40,-2.38,-2.40,-2.39,  nan,  nan,-2.37,  nan,-2.31,  nan,  nan,-2.39,  nan,  nan,-2.42,  nan,  nan,-2.41,-2.38,-2.47, lr 0.0004063093427071376 data time  0.18 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 49.85s | mean loss -2.16 | pos losses   nan,-0.34,  nan,-0.96,  nan,-1.16,  nan,-1.31,-1.49,-1.53,-1.58,  nan,  nan,  nan,-1.75,  nan,-1.82,-1.91,-1.89,  nan,-1.99,  nan,-1.99,  nan,-1.97,-2.09,-2.06,-2.05,  nan,  nan,-2.19,  nan,  nan,-2.19,-2.15,-2.19,-2.29,-2.19,  nan,  nan,  nan,  nan,-2.24,-2.26,-2.29,  nan,-2.30,-2.35,-2.34,-2.34,  nan,-2.28,-2.43,-2.36,-2.33,  nan,-2.25,-2.33,-2.39,-2.41,  nan,-2.39,  nan,  nan,-2.35,  nan,-2.42,-2.34,-2.40,-2.37,-2.36,  nan,-2.36,-2.27,  nan,  nan,  nan,  nan,  nan,-2.40,-2.41,  nan,-2.42,  nan,-2.38,-2.42,-2.37,-2.44,  nan,  nan,-2.44,-2.39,-2.43,-2.27,-2.36,-2.38,-2.34,-2.42,-2.42,  nan, lr 0.0003858245649446721 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 50.10s | mean loss -2.12 | pos losses   nan,  nan,-0.81,  nan,-1.03,-1.16,  nan,  nan,-1.39,-1.50,-1.56,-1.66,  nan,-1.74,-1.79,-1.77,  nan,-1.88,-1.93,-2.00,  nan,-2.01,  nan,  nan,-2.04,  nan,-2.13,  nan,-2.03,-2.12,  nan,-2.13,-2.17,-2.27,-2.21,-2.19,  nan,-2.20,-2.20,  nan,  nan,-2.26,-2.27,-2.21,  nan,  nan,-2.27,-2.26,-2.28,-2.32,-2.32,-2.35,-2.35,-2.29,-2.38,  nan,-2.40,-2.38,-2.25,  nan,-2.27,  nan,-2.36,-2.40,-2.41,-2.30,-2.36,  nan,-2.29,  nan,-2.40,  nan,-2.37,-2.40,-2.42,  nan,  nan,  nan,-2.40,-2.40,  nan,-2.39,  nan,-2.40,  nan,  nan,-2.36,  nan,-2.33,-2.36,  nan,-2.34,-2.44,-2.43,-2.44,-2.37,  nan,-2.39,-2.34,  nan, lr 0.00036554008969236717 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 50.06s | mean loss -2.10 | pos losses   nan,-0.30,  nan,-0.91,  nan,-1.18,-1.20,  nan,-1.47,-1.51,-1.59,-1.65,-1.70,-1.74,-1.76,-1.72,-1.72,-1.82,  nan,-1.98,-1.94,-1.96,-2.06,-1.99,  nan,-2.10,-2.08,-2.11,-2.14,-2.09,-2.10,  nan,  nan,-2.12,-2.16,-2.16,  nan,-2.29,-2.25,-2.26,-2.24,  nan,-2.29,  nan,  nan,-2.28,-2.29,-2.34,  nan,-2.36,-2.36,  nan,  nan,-2.24,  nan,-2.31,-2.36,  nan,  nan,-2.34,  nan,-2.39,  nan,-2.41,-2.40,  nan,-2.44,  nan,-2.44,-2.36,  nan,-2.32,  nan,-2.37,-2.28,  nan,  nan,-2.45,-2.41,-2.40,-2.44,-2.39,-2.32,  nan,  nan,-2.43,-2.39,-2.34,-2.48,-2.46,-2.36,  nan,  nan,-2.37,-2.39,  nan,  nan,-2.42,-2.39,-2.40, lr 0.00034549150281252633 data time  0.20 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 49.71s | mean loss -2.09 | pos losses   nan,-0.43,  nan,-0.98,-1.10,-1.17,  nan,-1.34,-1.39,-1.56,-1.58,-1.69,-1.64,-1.72,-1.76,  nan,-1.85,-1.89,-1.87,  nan,-1.98,  nan,  nan,-1.99,  nan,-2.04,-2.07,-2.10,-2.13,-2.17,  nan,-2.19,-2.19,-2.22,  nan,-2.22,-2.20,  nan,  nan,-2.28,-2.33,-2.26,-2.28,-2.26,  nan,  nan,-2.33,-2.36,-2.30,-2.32,-2.34,  nan,-2.39,  nan,-2.28,  nan,-2.42,-2.34,  nan,-2.36,-2.37,-2.35,  nan,  nan,-2.32,-2.37,  nan,-2.35,-2.38,-2.30,-2.38,  nan,  nan,-2.41,-2.39,-2.39,-2.42,-2.44,  nan,  nan,  nan,-2.44,-2.44,-2.39,-2.46,-2.36,-2.43,-2.34,-2.41,-2.37,-2.35,-2.39,-2.26,  nan,  nan,-2.47,-2.47,  nan,-2.32,  nan, lr 0.0003257139763390925 data time  0.29 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 50.10s | mean loss -2.05 | pos losses   nan,-0.39,  nan,-0.91,-1.05,-1.22,-1.31,  nan,-1.48,-1.56,  nan,-1.65,-1.69,  nan,-1.78,-1.82,-1.88,-1.87,  nan,-1.91,  nan,-2.00,-2.02,-2.03,-2.02,-2.06,  nan,-2.09,-2.15,-2.19,-2.11,  nan,  nan,-2.20,-2.18,-2.23,-2.24,  nan,  nan,-2.28,  nan,-2.22,-2.28,-2.26,  nan,-2.31,  nan,  nan,-2.32,-2.34,  nan,  nan,-2.33,-2.32,-2.33,-2.33,-2.38,-2.35,  nan,  nan,  nan,  nan,  nan,-2.45,  nan,  nan,  nan,-2.36,-2.42,  nan,-2.38,-2.37,  nan,-2.44,-2.38,  nan,  nan,  nan,-2.42,  nan,-2.42,-2.44,-2.44,-2.44,  nan,  nan,-2.39,-2.47,-2.38,-2.38,-2.45,-2.44,-2.38,-2.39,-2.40,-2.46,-2.48,-2.47,  nan,  nan, lr 0.0003062422067739485 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 49.83s | mean loss -2.18 | pos losses   nan,-0.39,  nan,-0.98,-1.05,-1.19,-1.33,-1.36,-1.45,  nan,  nan,  nan,  nan,  nan,-1.73,-1.86,  nan,-1.87,-1.95,  nan,-1.95,  nan,-2.03,-2.08,  nan,  nan,-2.13,-2.20,-2.14,  nan,  nan,  nan,  nan,  nan,-2.21,-2.19,-2.23,  nan,-2.27,-2.30,-2.31,  nan,-2.32,-2.26,-2.36,  nan,-2.31,-2.31,-2.32,-2.28,  nan,-2.34,  nan,-2.37,  nan,  nan,-2.42,-2.39,-2.34,  nan,  nan,  nan,-2.37,-2.39,-2.32,-2.32,-2.32,-2.35,  nan,  nan,-2.42,  nan,-2.44,-2.39,  nan,-2.44,-2.37,-2.35,-2.38,-2.42,-2.40,  nan,-2.41,  nan,-2.40,-2.44,-2.40,-2.41,-2.43,-2.40,-2.42,-2.46,  nan,-2.44,-2.42,  nan,  nan,-2.47,  nan,-2.46, lr 0.00028711035421746366 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 50.13s | mean loss -2.09 | pos losses   nan,  nan,  nan,-0.98,-1.13,  nan,  nan,-1.40,-1.49,-1.48,  nan,-1.62,-1.67,-1.69,-1.76,  nan,  nan,  nan,  nan,-1.96,  nan,-1.95,-2.10,-2.03,-2.04,-2.11,  nan,-2.17,-2.14,-2.09,  nan,-2.17,  nan,-2.20,-2.18,-2.27,  nan,-2.26,-2.15,-2.31,-2.28,  nan,-2.23,-2.22,-2.30,  nan,-2.28,-2.24,  nan,  nan,-2.36,-2.31,-2.38,  nan,-2.33,  nan,  nan,-2.36,-2.42,-2.36,-2.35,  nan,  nan,-2.37,-2.34,-2.39,-2.44,-2.34,-2.37,-2.35,-2.35,-2.38,-2.41,-2.39,-2.38,  nan,-2.43,  nan,  nan,-2.33,-2.45,-2.39,  nan,-2.39,-2.38,  nan,  nan,  nan,  nan,-2.35,-2.41,-2.45,-2.41,  nan,  nan,  nan,  nan,-2.37,-2.46,-2.38, lr 0.00026835198244006924 data time  0.16 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 50.00s | mean loss -2.16 | pos losses   nan,  nan,-0.80,  nan,-1.06,-1.20,-1.33,  nan,-1.43,-1.50,-1.66,-1.68,-1.74,  nan,  nan,-1.83,-1.86,  nan,-1.91,  nan,-1.96,-2.02,-2.02,  nan,-1.99,-2.10,  nan,-2.17,  nan,-2.07,-2.14,-2.20,  nan,  nan,-2.27,  nan,-2.28,  nan,  nan,  nan,-2.30,-2.25,  nan,-2.30,-2.28,-2.25,-2.36,  nan,-2.35,-2.33,-2.32,-2.29,  nan,-2.34,  nan,  nan,-2.33,-2.37,  nan,-2.36,-2.35,-2.38,  nan,-2.39,-2.44,-2.36,-2.36,-2.38,-2.36,-2.50,-2.40,-2.41,-2.32,  nan,  nan,-2.34,  nan,-2.38,-2.44,  nan,  nan,-2.42,-2.44,  nan,-2.44,  nan,  nan,-2.39,  nan,-2.42,  nan,  nan,-2.38,  nan,-2.41,-2.39,-2.45,  nan,  nan,-2.40, lr 0.0002500000000000001 data time  0.31 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 49.52s | mean loss -2.15 | pos losses   nan,-0.37,-0.82,-0.94,-1.11,-1.14,-1.36,-1.37,-1.45,  nan,  nan,-1.66,-1.66,  nan,-1.75,-1.83,-1.98,  nan,  nan,  nan,-2.02,-1.99,-2.01,-2.05,  nan,  nan,-2.05,-2.05,-2.16,  nan,-2.14,  nan,-2.19,-2.24,-2.26,-2.22,-2.25,-2.21,-2.24,  nan,-2.29,  nan,  nan,  nan,-2.31,-2.26,  nan,-2.33,-2.32,-2.31,-2.42,-2.29,  nan,  nan,-2.39,-2.41,-2.41,  nan,  nan,-2.36,  nan,-2.40,-2.48,  nan,  nan,  nan,-2.42,  nan,-2.42,  nan,  nan,-2.40,-2.40,-2.44,  nan,  nan,-2.39,-2.45,-2.44,-2.43,-2.37,-2.36,-2.39,-2.39,-2.43,-2.43,-2.44,-2.39,-2.41,  nan,-2.47,  nan,  nan,  nan,-2.45,  nan,-2.41,-2.45,  nan,-2.50, lr 0.00023208660251050156 data time  0.20 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 50.14s | mean loss -2.18 | pos losses   nan,  nan,-0.82,  nan,-1.11,-1.25,-1.36,-1.41,-1.48,  nan,-1.63,-1.64,  nan,  nan,-1.82,-1.83,-1.88,-1.85,-1.92,  nan,  nan,-2.02,-1.99,-2.07,  nan,-2.09,-2.10,  nan,  nan,-2.20,  nan,-2.17,  nan,  nan,-2.21,-2.18,-2.25,  nan,-2.25,-2.33,  nan,-2.32,  nan,-2.22,-2.31,  nan,-2.32,-2.32,-2.34,  nan,  nan,  nan,  nan,-2.30,-2.33,-2.35,-2.33,-2.40,  nan,  nan,-2.36,-2.39,-2.40,  nan,  nan,  nan,-2.40,-2.41,  nan,  nan,-2.41,-2.39,-2.52,-2.38,-2.44,  nan,-2.40,-2.42,-2.33,-2.37,-2.36,-2.38,-2.46,  nan,-2.42,-2.34,-2.41,  nan,  nan,-2.47,-2.43,  nan,-2.46,-2.41,-2.48,-2.47,-2.35,-2.43,  nan,-2.53, lr 0.0002146432161577842 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 50.11s | mean loss -2.25 | pos losses   nan,-0.38,-0.85,  nan,-1.06,  nan,-1.24,  nan,  nan,-1.53,  nan,-1.69,  nan,  nan,  nan,-1.84,  nan,-1.92,-1.87,  nan,-2.03,  nan,-2.03,  nan,-2.10,  nan,-2.13,-2.12,-2.16,  nan,  nan,-2.16,-2.15,-2.24,-2.20,  nan,-2.26,  nan,-2.30,-2.25,  nan,  nan,  nan,  nan,-2.32,-2.30,-2.33,-2.35,  nan,-2.30,-2.33,-2.39,  nan,-2.37,-2.34,-2.39,-2.43,-2.40,-2.42,-2.42,  nan,  nan,  nan,-2.38,  nan,-2.43,  nan,-2.43,-2.39,-2.42,  nan,-2.41,-2.45,-2.39,  nan,  nan,  nan,-2.43,-2.41,-2.45,  nan,-2.46,-2.39,-2.45,-2.54,-2.43,-2.40,-2.45,-2.34,-2.48,-2.43,-2.56,-2.44,-2.40,-2.52,  nan,  nan,-2.40,  nan,-2.40, lr 0.00019770044256881258 data time  0.16 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 50.21s | mean loss -2.19 | pos losses   nan,  nan,-0.83,-0.93,-1.14,-1.21,-1.28,  nan,-1.41,-1.60,-1.57,-1.69,-1.76,-1.78,-1.79,-1.85,-1.89,  nan,  nan,-1.96,-2.00,-1.98,-2.12,  nan,  nan,-2.14,  nan,-2.09,  nan,-2.17,  nan,-2.16,  nan,-2.16,-2.24,-2.20,  nan,-2.21,-2.29,-2.20,-2.33,-2.33,  nan,-2.35,-2.32,-2.32,-2.32,  nan,  nan,-2.39,  nan,-2.38,-2.37,  nan,  nan,-2.35,-2.37,-2.41,-2.34,-2.41,  nan,-2.41,-2.39,-2.46,  nan,-2.41,  nan,  nan,  nan,-2.38,  nan,-2.39,-2.42,  nan,  nan,-2.44,  nan,  nan,-2.48,-2.45,-2.46,-2.43,  nan,  nan,-2.40,-2.45,  nan,-2.50,-2.49,-2.41,-2.41,-2.43,  nan,-2.41,-2.52,  nan,-2.46,  nan,  nan,-2.49, lr 0.00018128800512565513 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 49.99s | mean loss -2.16 | pos losses   nan,-0.41,-0.85,  nan,  nan,-1.18,-1.28,-1.41,-1.49,-1.58,  nan,-1.65,  nan,-1.73,-1.78,  nan,  nan,-1.90,-1.94,-1.93,  nan,-1.99,-2.08,-2.05,-2.05,-2.08,  nan,-2.11,  nan,-2.14,-2.13,  nan,-2.16,  nan,-2.19,-2.25,  nan,-2.23,-2.26,-2.26,  nan,  nan,  nan,  nan,  nan,  nan,-2.33,-2.35,-2.39,  nan,-2.35,-2.39,-2.36,-2.33,-2.41,-2.42,-2.42,-2.36,  nan,-2.42,  nan,-2.35,-2.46,-2.34,-2.47,  nan,  nan,-2.46,-2.42,-2.39,  nan,  nan,  nan,-2.39,-2.35,-2.43,  nan,-2.49,  nan,-2.42,  nan,-2.40,-2.44,  nan,  nan,-2.37,-2.41,  nan,  nan,-2.47,-2.47,-2.43,-2.49,  nan,-2.42,  nan,-2.42,-2.44,-2.41,  nan, lr 0.00016543469682057105 data time  0.30 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 49.69s | mean loss -2.18 | pos losses   nan,  nan,-0.82,  nan,  nan,-1.23,-1.32,  nan,-1.49,  nan,-1.64,-1.62,-1.77,-1.76,-1.78,-1.84,-1.87,  nan,  nan,-1.89,-1.98,  nan,  nan,-2.04,-2.07,-2.06,-2.13,-2.13,-2.14,-2.20,-2.12,-2.25,  nan,-2.22,  nan,  nan,  nan,-2.25,-2.32,-2.23,-2.28,-2.29,-2.38,-2.32,  nan,-2.33,-2.32,  nan,-2.39,-2.40,-2.38,  nan,-2.41,-2.39,  nan,-2.38,-2.33,-2.39,-2.43,-2.41,  nan,-2.39,  nan,-2.42,-2.43,-2.44,  nan,-2.42,-2.42,-2.47,-2.45,-2.41,  nan,-2.51,  nan,-2.43,-2.43,-2.42,  nan,  nan,-2.45,-2.46,-2.44,-2.43,  nan,  nan,-2.41,-2.43,-2.46,-2.47,-2.51,-2.46,  nan,-2.41,-2.42,  nan,  nan,-2.38,-2.50,-2.49, lr 0.00015016832974331724 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 50.10s | mean loss -2.11 | pos losses   nan,-0.48,  nan,  nan,-1.03,-1.22,-1.32,  nan,-1.47,-1.54,-1.65,-1.72,  nan,-1.83,-1.75,-1.85,-1.87,-1.92,  nan,-1.96,  nan,-2.01,-2.04,-1.99,-2.11,-2.06,-2.13,  nan,  nan,  nan,  nan,  nan,-2.20,-2.20,  nan,-2.21,-2.26,  nan,-2.23,-2.27,-2.35,  nan,  nan,-2.34,-2.30,  nan,  nan,-2.33,  nan,-2.34,  nan,-2.32,-2.36,-2.38,-2.35,-2.36,-2.40,-2.40,  nan,  nan,-2.41,  nan,  nan,-2.43,-2.44,  nan,-2.43,  nan,  nan,  nan,  nan,  nan,-2.41,-2.46,  nan,-2.43,  nan,-2.41,  nan,-2.51,  nan,  nan,-2.36,-2.48,  nan,  nan,  nan,-2.41,  nan,-2.39,-2.45,-2.45,  nan,  nan,-2.37,  nan,  nan,-2.45,-2.42,-2.43, lr 0.00013551568628929433 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 50.19s | mean loss -2.20 | pos losses   nan,  nan,  nan,  nan,-1.11,-1.24,-1.32,-1.41,  nan,-1.58,  nan,  nan,-1.69,  nan,  nan,-1.81,-1.94,-1.89,-1.89,  nan,  nan,-2.01,  nan,  nan,-2.10,-2.10,-2.15,-2.14,-2.17,-2.18,-2.20,-2.22,-2.18,  nan,-2.21,-2.28,  nan,-2.20,  nan,  nan,-2.30,-2.25,-2.32,-2.33,  nan,  nan,-2.34,-2.34,  nan,-2.39,-2.32,-2.36,-2.33,-2.37,  nan,-2.40,  nan,-2.39,-2.36,  nan,  nan,-2.47,  nan,-2.38,  nan,-2.41,  nan,-2.46,-2.42,-2.43,-2.46,-2.45,-2.40,  nan,  nan,-2.48,-2.38,-2.39,-2.45,-2.46,  nan,  nan,  nan,-2.44,-2.47,-2.47,-2.46,-2.41,-2.44,  nan,-2.43,  nan,-2.44,-2.37,-2.45,  nan,-2.47,  nan,-2.49,  nan, lr 0.00012150247217412185 data time  0.17 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 50.88s | mean loss -2.12 | pos losses   nan,-0.44,-0.80,-0.95,-1.11,-1.20,-1.32,-1.42,-1.51,-1.57,  nan,  nan,-1.67,-1.69,-1.77,-1.77,-1.81,-1.91,-1.95,  nan,-1.97,  nan,  nan,  nan,-2.07,-2.10,-2.14,  nan,-2.17,  nan,  nan,-2.22,  nan,-2.17,-2.28,  nan,-2.24,-2.28,-2.30,-2.29,  nan,-2.31,-2.37,  nan,-2.36,-2.31,-2.28,  nan,-2.35,-2.35,  nan,  nan,  nan,-2.40,-2.42,-2.38,-2.43,  nan,  nan,  nan,-2.41,-2.41,-2.42,  nan,  nan,-2.49,-2.42,-2.41,-2.42,-2.45,-2.39,  nan,-2.42,  nan,  nan,  nan,-2.52,  nan,  nan,-2.43,-2.43,  nan,-2.44,-2.42,-2.39,-2.45,-2.43,-2.46,  nan,-2.44,  nan,  nan,-2.50,-2.49,  nan,-2.39,-2.50,-2.42,  nan,  nan, lr 0.00010815327133708014 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 50.27s | mean loss -2.20 | pos losses   nan,-0.51,-0.78,-0.96,-1.14,  nan,-1.33,-1.39,  nan,  nan,-1.64,  nan,  nan,  nan,  nan,-1.84,-1.93,  nan,-1.93,-1.94,-1.95,  nan,-2.04,-2.05,  nan,-2.04,-2.21,-2.14,-2.11,  nan,-2.15,-2.20,-2.20,-2.21,  nan,-2.28,-2.33,-2.25,-2.22,  nan,-2.31,  nan,-2.28,  nan,-2.37,-2.33,-2.39,-2.39,-2.37,-2.36,-2.40,-2.37,-2.34,  nan,  nan,  nan,  nan,  nan,  nan,-2.35,-2.41,  nan,-2.38,  nan,-2.42,  nan,-2.45,-2.40,-2.47,-2.43,-2.44,-2.44,-2.42,  nan,  nan,  nan,-2.46,  nan,-2.49,  nan,-2.49,-2.40,-2.48,-2.44,-2.41,  nan,-2.51,-2.47,-2.48,-2.42,-2.45,-2.46,-2.42,-2.48,-2.46,-2.50,-2.44,-2.46,-2.44,  nan, lr 9.549150281252633e-05 data time  0.31 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 49.83s | mean loss -2.22 | pos losses   nan,-0.38,  nan,-1.04,  nan,  nan,-1.33,  nan,-1.53,  nan,-1.61,-1.66,  nan,-1.77,  nan,-1.85,  nan,  nan,-1.93,  nan,-1.95,  nan,-2.08,  nan,-2.09,  nan,-2.12,-2.15,  nan,-2.13,-2.20,-2.16,-2.24,-2.20,  nan,-2.19,  nan,-2.24,-2.25,-2.29,  nan,-2.26,  nan,-2.33,  nan,-2.35,-2.39,  nan,  nan,  nan,-2.39,-2.39,-2.35,  nan,  nan,-2.39,-2.43,-2.40,  nan,-2.35,  nan,-2.40,  nan,-2.36,  nan,-2.46,  nan,-2.46,  nan,-2.43,  nan,-2.43,  nan,-2.40,  nan,-2.43,  nan,-2.41,-2.48,-2.47,-2.42,  nan,-2.39,-2.43,  nan,  nan,-2.47,-2.46,-2.45,-2.45,-2.44,-2.44,  nan,-2.48,  nan,-2.47,-2.42,-2.48,-2.48,  nan, lr 8.353937964495028e-05 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 50.69s | mean loss -2.22 | pos losses   nan,-0.48,-0.87,-0.96,  nan,  nan,-1.35,-1.36,  nan,-1.47,-1.60,-1.74,-1.72,-1.78,-1.81,-1.87,-1.93,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.09,  nan,-2.11,-2.21,-2.19,-2.19,  nan,-2.19,  nan,-2.23,  nan,-2.24,  nan,  nan,-2.26,-2.30,  nan,  nan,-2.32,-2.31,-2.33,  nan,-2.33,-2.38,  nan,  nan,-2.34,-2.38,-2.34,-2.38,-2.37,-2.41,-2.47,  nan,-2.38,  nan,-2.41,  nan,-2.39,-2.40,-2.46,-2.41,  nan,  nan,  nan,-2.42,-2.37,  nan,-2.47,-2.44,-2.39,-2.45,  nan,-2.46,-2.47,  nan,  nan,-2.46,-2.48,-2.46,-2.42,  nan,  nan,-2.43,  nan,-2.47,-2.37,-2.45,-2.44,  nan,-2.49,-2.44,  nan,-2.48,-2.50,  nan, lr 7.23178699197467e-05 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 50.62s | mean loss -2.26 | pos losses   nan,  nan,  nan,-0.97,-1.10,-1.22,  nan,-1.33,-1.56,  nan,-1.55,  nan,-1.73,  nan,-1.84,  nan,-1.82,-1.84,-1.93,-1.97,-2.02,  nan,  nan,-2.08,-2.07,  nan,-2.08,-2.25,  nan,-2.15,  nan,-2.20,-2.22,  nan,-2.26,  nan,-2.25,-2.21,-2.33,-2.28,-2.30,-2.33,-2.32,  nan,-2.35,-2.37,-2.34,-2.25,-2.36,  nan,-2.37,-2.36,-2.43,  nan,  nan,  nan,  nan,-2.35,-2.41,-2.45,-2.44,-2.49,-2.47,-2.41,-2.39,-2.40,  nan,-2.47,  nan,  nan,-2.48,-2.38,  nan,-2.50,-2.41,-2.44,-2.46,-2.42,-2.40,  nan,-2.44,-2.45,-2.44,-2.43,-2.41,-2.44,  nan,  nan,-2.41,-2.50,-2.39,-2.45,-2.47,  nan,-2.53,-2.46,-2.44,-2.46,-2.53,  nan, lr 6.184665997806832e-05 data time  0.19 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 50.75s | mean loss -2.15 | pos losses   nan,-0.45,-0.84,-0.97,-1.14,-1.24,  nan,-1.48,-1.50,-1.53,-1.65,-1.64,-1.71,-1.74,  nan,-1.87,-1.87,-1.92,-1.95,-2.04,  nan,-1.96,-2.07,-2.03,-2.14,-2.14,-2.10,-2.16,  nan,  nan,-2.17,-2.21,-2.25,-2.30,-2.26,  nan,  nan,-2.28,  nan,  nan,-2.27,  nan,-2.40,-2.31,  nan,-2.32,-2.41,  nan,  nan,  nan,-2.46,  nan,-2.34,-2.35,  nan,-2.40,  nan,  nan,  nan,-2.36,-2.40,-2.45,-2.45,  nan,  nan,  nan,-2.44,-2.40,  nan,-2.44,-2.42,-2.41,-2.41,-2.42,  nan,-2.40,  nan,  nan,-2.42,-2.41,  nan,-2.47,-2.45,  nan,-2.43,-2.52,  nan,  nan,  nan,  nan,  nan,-2.45,-2.51,-2.50,  nan,-2.50,-2.47,-2.41,-2.47,  nan, lr 5.214411988029355e-05 data time  0.20 step time  0.11 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 50.04s | mean loss -2.13 | pos losses   nan,-0.47,-0.83,-0.95,-1.04,  nan,-1.30,-1.39,-1.50,-1.57,-1.55,-1.69,-1.65,  nan,  nan,-1.83,-1.83,-1.87,-1.88,  nan,-1.98,-2.00,-2.03,-2.01,-2.10,  nan,  nan,  nan,  nan,-2.15,-2.17,  nan,-2.25,-2.18,  nan,  nan,  nan,  nan,  nan,-2.28,-2.35,  nan,  nan,  nan,-2.36,-2.37,  nan,-2.43,-2.35,-2.38,-2.36,  nan,  nan,-2.41,-2.34,-2.45,-2.36,-2.42,  nan,  nan,  nan,-2.45,-2.43,  nan,  nan,  nan,-2.44,-2.45,-2.42,-2.41,-2.41,-2.47,-2.45,-2.41,-2.42,-2.45,  nan,  nan,-2.43,  nan,-2.43,  nan,-2.42,-2.45,-2.44,-2.50,  nan,-2.50,-2.51,-2.43,-2.43,  nan,  nan,-2.48,-2.53,  nan,-2.49,  nan,  nan,-2.51, lr 4.322727117869951e-05 data time  0.29 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 49.66s | mean loss -2.22 | pos losses   nan,  nan,-0.79,  nan,  nan,  nan,-1.33,-1.41,  nan,-1.55,-1.58,  nan,-1.74,-1.78,  nan,-1.81,-1.90,-1.93,  nan,-2.00,-1.98,  nan,-2.04,-2.03,-2.13,  nan,-2.15,-2.12,  nan,-2.19,-2.16,-2.20,  nan,-2.23,-2.20,-2.25,-2.26,-2.26,-2.32,  nan,  nan,  nan,-2.31,-2.35,-2.30,-2.36,-2.31,-2.35,-2.43,  nan,  nan,-2.41,-2.40,  nan,-2.33,-2.39,  nan,-2.43,-2.42,  nan,  nan,-2.31,-2.38,-2.40,  nan,  nan,-2.44,-2.44,  nan,-2.41,-2.41,  nan,-2.46,  nan,-2.44,  nan,  nan,  nan,-2.47,-2.38,  nan,  nan,-2.47,-2.45,-2.47,-2.46,-2.46,-2.49,  nan,-2.41,  nan,  nan,  nan,-2.47,  nan,  nan,-2.47,-2.50,-2.53,-2.43, lr 3.5111757055874326e-05 data time  0.18 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 50.07s | mean loss -2.11 | pos losses   nan,-0.51,  nan,-0.94,-1.12,-1.24,-1.35,-1.39,-1.46,-1.62,-1.59,  nan,-1.72,-1.75,  nan,-1.87,-1.83,  nan,-1.95,  nan,  nan,-2.11,-2.00,-2.06,-2.09,-2.11,-2.15,-2.10,-2.21,  nan,-2.22,  nan,-2.20,  nan,-2.24,-2.25,-2.22,-2.36,  nan,  nan,-2.33,-2.31,-2.32,  nan,-2.41,  nan,-2.38,-2.36,  nan,  nan,-2.39,  nan,  nan,  nan,  nan,  nan,-2.39,  nan,  nan,-2.49,  nan,  nan,  nan,-2.39,-2.38,-2.42,-2.47,-2.43,-2.45,-2.44,-2.45,  nan,-2.44,-2.42,  nan,  nan,-2.46,  nan,  nan,-2.46,-2.42,  nan,  nan,-2.47,  nan,-2.44,  nan,  nan,  nan,-2.38,  nan,-2.50,-2.50,-2.43,-2.46,  nan,  nan,-2.43,-2.42,-2.52, lr 2.78118148812595e-05 data time  0.17 step time  0.12 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 49.95s | mean loss -2.19 | pos losses   nan,  nan,-0.84,  nan,-1.15,  nan,-1.37,  nan,-1.45,-1.54,-1.59,  nan,-1.66,-1.81,  nan,-1.81,-1.88,  nan,-1.90,  nan,  nan,-2.10,-2.09,-2.07,-2.05,-2.15,  nan,-2.12,  nan,-2.19,-2.17,-2.19,-2.20,-2.21,-2.27,-2.23,-2.26,-2.28,  nan,-2.27,-2.30,-2.30,  nan,-2.31,-2.32,-2.33,-2.35,  nan,  nan,-2.38,-2.38,-2.42,-2.35,-2.41,  nan,  nan,-2.34,-2.37,-2.41,  nan,-2.37,-2.43,  nan,-2.49,  nan,-2.40,-2.44,  nan,-2.42,-2.43,-2.53,  nan,  nan,  nan,-2.48,-2.40,-2.47,  nan,-2.44,  nan,-2.39,-2.48,  nan,-2.44,  nan,  nan,  nan,-2.44,-2.42,-2.43,  nan,-2.51,-2.47,  nan,-2.45,-2.46,  nan,  nan,-2.43,-2.52, lr 2.134025123396638e-05 data time  0.21 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 50.15s | mean loss -2.17 | pos losses   nan,-0.44,-0.82,  nan,  nan,  nan,-1.32,-1.40,  nan,-1.56,  nan,-1.69,-1.74,-1.70,-1.77,-1.82,  nan,  nan,-1.99,-1.99,-2.00,-2.03,-2.01,-2.05,-2.07,-2.05,  nan,  nan,  nan,-2.17,-2.19,-2.14,-2.22,-2.18,-2.26,  nan,  nan,-2.25,-2.32,  nan,  nan,-2.28,-2.32,  nan,-2.34,-2.30,-2.37,  nan,-2.37,-2.39,-2.30,-2.44,-2.43,-2.43,-2.39,-2.41,-2.41,  nan,-2.37,-2.41,  nan,  nan,-2.38,  nan,-2.42,-2.45,  nan,-2.42,-2.45,-2.50,  nan,-2.45,-2.35,  nan,  nan,-2.43,  nan,-2.46,-2.45,  nan,-2.49,-2.43,-2.45,  nan,-2.50,-2.49,  nan,-2.46,-2.45,-2.49,-2.50,  nan,-2.47,  nan,  nan,-2.44,-2.41,  nan,-2.44,  nan, lr 1.5708419435684463e-05 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 49.69s | mean loss -2.25 | pos losses   nan,-0.50,-0.85,  nan,-1.09,-1.27,-1.24,-1.46,  nan,-1.55,  nan,-1.63,  nan,-1.81,  nan,-1.82,  nan,  nan,-1.92,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-2.19,-2.14,  nan,-2.19,-2.23,  nan,  nan,  nan,-2.29,-2.26,  nan,-2.31,-2.29,-2.36,-2.29,  nan,-2.35,-2.33,-2.40,  nan,-2.28,-2.41,  nan,-2.40,-2.37,  nan,-2.40,-2.40,-2.42,-2.36,-2.40,-2.39,  nan,-2.43,  nan,  nan,  nan,-2.44,-2.45,-2.41,-2.42,  nan,-2.46,-2.45,-2.41,  nan,-2.38,  nan,-2.47,-2.42,-2.42,-2.43,-2.43,-2.48,  nan,-2.48,-2.47,-2.52,-2.47,-2.44,-2.56,-2.48,-2.52,-2.50,-2.43,  nan,-2.51,-2.46,-2.47,-2.51,-2.54,-2.48, lr 1.0926199633097156e-05 data time  0.29 step time  0.12 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 50.24s | mean loss -2.16 | pos losses   nan,  nan,-0.83,-1.02,-1.12,-1.23,-1.31,  nan,  nan,  nan,-1.68,-1.67,-1.74,  nan,-1.79,  nan,-1.87,-1.90,-1.95,-1.92,-2.01,  nan,  nan,  nan,  nan,-2.05,-2.09,-2.12,-2.20,-2.19,  nan,-2.20,-2.18,-2.27,-2.27,-2.33,-2.25,-2.30,  nan,-2.28,-2.30,  nan,-2.34,-2.34,-2.30,-2.39,  nan,-2.30,  nan,-2.38,-2.38,-2.39,  nan,  nan,-2.38,-2.37,  nan,-2.39,  nan,-2.37,-2.48,-2.40,  nan,  nan,  nan,-2.44,-2.44,-2.49,-2.47,-2.41,  nan,  nan,  nan,-2.46,-2.40,-2.51,  nan,-2.48,  nan,-2.53,-2.47,  nan,-2.48,-2.47,  nan,  nan,-2.49,-2.48,-2.43,  nan,  nan,-2.42,-2.44,-2.38,  nan,-2.48,-2.49,-2.44,  nan,  nan, lr 7.001981464747565e-06 data time  0.17 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 50.28s | mean loss -2.16 | pos losses   nan,-0.42,-0.82,-1.08,-1.14,-1.24,-1.37,-1.41,-1.40,-1.64,-1.61,-1.60,-1.78,-1.73,-1.77,-1.77,  nan,  nan,  nan,  nan,-2.03,-2.07,  nan,-2.11,  nan,-2.15,-2.07,-2.12,-2.19,-2.13,-2.21,-2.23,-2.21,-2.26,-2.25,-2.29,  nan,-2.32,-2.26,-2.32,-2.34,-2.31,  nan,-2.30,-2.38,  nan,  nan,-2.38,-2.41,-2.32,  nan,-2.33,  nan,-2.36,  nan,-2.43,-2.45,-2.46,-2.45,-2.43,-2.48,-2.39,-2.37,  nan,-2.44,-2.44,-2.42,-2.46,-2.45,-2.49,-2.44,  nan,  nan,  nan,  nan,-2.42,  nan,  nan,-2.48,-2.45,-2.47,-2.49,-2.42,-2.45,-2.41,  nan,-2.46,-2.50,  nan,-2.43,-2.45,  nan,  nan,-2.45,-2.47,-2.55,  nan,-2.43,  nan,  nan, lr 3.942649342761117e-06 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 50.14s | mean loss -2.16 | pos losses   nan,  nan,-0.81,-1.00,-1.12,-1.20,  nan,-1.40,-1.46,-1.59,-1.62,  nan,-1.69,-1.78,  nan,-1.84,  nan,-1.87,-1.93,-1.91,  nan,  nan,-2.05,  nan,-2.05,-2.15,  nan,  nan,  nan,-2.20,  nan,-2.24,  nan,-2.20,  nan,-2.26,-2.28,-2.24,-2.29,-2.39,-2.37,  nan,-2.32,-2.32,  nan,-2.33,-2.35,-2.34,  nan,-2.38,  nan,  nan,-2.39,-2.40,  nan,-2.44,-2.43,-2.44,-2.43,  nan,-2.48,-2.46,  nan,-2.42,-2.44,  nan,  nan,  nan,-2.47,-2.51,-2.38,  nan,-2.43,  nan,-2.45,-2.43,-2.42,-2.42,-2.43,  nan,-2.46,-2.47,  nan,  nan,-2.45,  nan,  nan,-2.50,-2.46,-2.40,-2.50,-2.46,  nan,  nan,-2.43,  nan,-2.50,  nan,  nan,-2.51, lr 1.753570375247815e-06 data time  0.19 step time  0.11 forward time  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 50.47s | mean loss -2.17 | pos losses   nan,-0.48,-0.84,-1.02,-1.15,  nan,  nan,-1.45,  nan,  nan,-1.67,-1.75,  nan,-1.77,-1.88,  nan,  nan,-1.86,-2.00,-2.01,  nan,-2.08,-2.02,-2.04,-2.06,-2.11,-2.12,-2.18,  nan,  nan,-2.17,  nan,-2.21,  nan,-2.19,-2.30,-2.27,-2.27,-2.27,-2.30,-2.30,-2.26,-2.27,-2.31,-2.35,  nan,-2.31,-2.35,-2.37,-2.37,-2.38,-2.32,-2.40,  nan,  nan,-2.40,-2.42,-2.38,-2.41,-2.43,-2.37,  nan,  nan,  nan,-2.41,-2.45,-2.50,-2.40,  nan,-2.45,-2.42,  nan,  nan,-2.40,  nan,  nan,  nan,-2.42,-2.48,-2.44,-2.48,-2.42,  nan,  nan,  nan,  nan,-2.42,  nan,-2.44,  nan,  nan,  nan,  nan,-2.51,-2.51,-2.47,  nan,-2.41,  nan,-2.43, lr 4.3858495057080837e-07 data time  0.32 step time  0.12 forward time  0.01\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "get_batch_func = lcpfn.create_get_batch_func(prior=lcpfn.sample_from_prior)\n",
        "\n",
        "# They tried\n",
        "# emsize ∈ [128, 256, 512]\n",
        "# nlayers ∈ [3, 6, 12]\n",
        "# nb_data ∈ [100k, 1M, 10M]\n",
        "# steps_per_epoch is hardcoded to 100\n",
        "# num_epochs?\n",
        "\n",
        "result = lcpfn.train_lcpfn(get_batch_func=get_batch_func,\n",
        "                          seq_len=100,\n",
        "                         emsize=256,\n",
        "                         nlayers=3,\n",
        "                         num_borders=1000,\n",
        "                         lr=0.001,\n",
        "                         batch_size=500,\n",
        "                         epochs=100)\n",
        "\n",
        "transformer_model = result[2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPNYY6hNjQx8"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"lcpfn/trained_models/\"+\"reproduction_model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e8WB2zQjQx9"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "\n",
        "torch.save(transformer_model, model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkMiFLOjQx-"
      },
      "source": [
        "# Getting the lc data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qv8ESnYE4uvI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the data for cutoff 10 inference\n",
        "prior_10 = lcpfn.sample_from_prior(np.random)\n",
        "curve_10, _ = prior_10()\n",
        "x_10 = torch.arange(1, 101).unsqueeze(1)\n",
        "y_10 = torch.from_numpy(curve_10).float().unsqueeze(1)\n",
        "cutoff_10 = 10\n",
        "data_10 = {'x': x_10, 'y': y_10, 'cutoff': cutoff_10}\n",
        "\n",
        "# Get the data for cutoff 20 inference\n",
        "prior_20 = lcpfn.sample_from_prior(np.random)\n",
        "curve_20, _ = prior_20()\n",
        "x_20 = torch.arange(1, 101).unsqueeze(1)\n",
        "y_20 = torch.from_numpy(curve_20).float().unsqueeze(1)\n",
        "cutoff_20 = 20\n",
        "data_20 = {'x': x_20, 'y': y_20, 'cutoff': cutoff_20}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9caSWD1jQyC"
      },
      "source": [
        "# Inference with LCPFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vbdzaax5ptK"
      },
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "lcpfn_model = lcpfn.LCPFN(model_save_path)\n",
        "\n",
        "# Predictions for cutoff = 10\n",
        "x = data_10['x']\n",
        "y = data_10['y']\n",
        "cutoff = data_10['cutoff']\n",
        "predictions_10 = lcpfn_model.predict_quantiles(x_train=x[:cutoff], y_train=y[:cutoff], x_test=x[cutoff:], qs=[0.05, 0.5, 0.95])\n",
        "\n",
        "# Predictions for cutoff = 20\n",
        "x = data_20['x']\n",
        "y = data_20['y']\n",
        "cutoff = data_20['cutoff']\n",
        "predictions_20 = lcpfn_model.predict_quantiles(x_train=x[:cutoff], y_train=y[:cutoff], x_test=x[cutoff:], qs=[0.05, 0.5, 0.95])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "5Jikycd-7CV_",
        "outputId": "43f47547-62a2-429e-9169-56cb1feea1ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Y')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot target curves\n",
        "plt.plot(x_10, y_10, \"black\", label=\"Target for Cutoff 10\")\n",
        "plt.plot(x_20, y_20, \"grey\", label=\"Target for Cutoff 20\")\n",
        "\n",
        "# Plot extrapolations\n",
        "plt.plot(x_10[cutoff_10:], predictions_10[:, 1], \"blue\", label=\"Extrapolation for Cutoff 10\")\n",
        "plt.fill_between(x_10[cutoff_10:].flatten(), predictions_10[:, 0], predictions_10[:, 2], color=\"blue\", alpha=0.2, label=\"90% CI for Cutoff 10\")\n",
        "\n",
        "plt.plot(x_20[cutoff_20:], predictions_20[:, 1], \"green\", label=\"Extrapolation for Cutoff 20\")\n",
        "plt.fill_between(x_20[cutoff_20:].flatten(), predictions_20[:, 0], predictions_20[:, 2], color=\"green\", alpha=0.2, label=\"90% CI for Cutoff 20\")\n",
        "\n",
        "# Plot cutoff lines\n",
        "plt.axvline(x=cutoff_10, color='blue', linestyle='--', linewidth=0.5, label='Cutoff at 10')\n",
        "plt.axvline(x=cutoff_20, color='green', linestyle='--', linewidth=0.5, label='Cutoff at 20')\n",
        "\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Model Extrapolation with Different Cutoffs\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGTs0HtBjQyE"
      },
      "source": [
        "# Inference with MCMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "XCEK9mnHjQyE",
        "outputId": "e419c1c6-7994-4756-c2cd-85c39ab3c0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /home/antonio14bernardes/.local/lib/python3.8/site-packages (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=4.8.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/lib/python3/dist-packages (from torch) (2.10.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied, skipping upgrade: fsspec in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied, skipping upgrade: sympy in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-nvjitlink-cu12 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.3.101)\n",
            "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: gpytorch in /home/antonio14bernardes/.local/lib/python3.8/site-packages (1.10)\n",
            "Requirement already satisfied: scikit-learn in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from gpytorch) (1.3.2)\n",
            "Requirement already satisfied: linear-operator>=0.4.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from gpytorch) (0.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.11 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from linear-operator>=0.4.0->gpytorch) (2.2.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.0.106)\n",
            "Requirement already satisfied: fsspec in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2024.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: filelock in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: networkx in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (11.4.5.107)\n",
            "Requirement already satisfied: sympy in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.12)\n",
            "Requirement already satisfied: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.11->linear-operator>=0.4.0->gpytorch) (12.3.101)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from sympy->torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: botorch in /home/antonio14bernardes/.local/lib/python3.8/site-packages (0.8.5)\n",
            "Requirement already satisfied: linear-operator==0.4.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (0.4.0)\n",
            "Requirement already satisfied: gpytorch==1.10 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (1.10)\n",
            "Requirement already satisfied: torch>=1.12 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (2.2.2)\n",
            "Requirement already satisfied: pyro-ppl>=1.8.4 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (1.9.0)\n",
            "Requirement already satisfied: scipy in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (1.10.1)\n",
            "Requirement already satisfied: multipledispatch in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from gpytorch==1.10->botorch) (1.3.2)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (11.4.5.107)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (2.19.3)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.12->botorch) (2.10.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (8.9.2.26)\n",
            "Requirement already satisfied: fsspec in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (2024.2.0)\n",
            "Requirement already satisfied: networkx in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (3.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.105)\n",
            "Requirement already satisfied: sympy in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (12.1.105)\n",
            "Requirement already satisfied: filelock in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from torch>=1.12->botorch) (3.13.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from pyro-ppl>=1.8.4->botorch) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.36 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from pyro-ppl>=1.8.4->botorch) (4.66.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch==1.10->botorch) (3.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from scikit-learn->gpytorch==1.10->botorch) (1.3.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.12->botorch) (12.3.101)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/antonio14bernardes/.local/lib/python3.8/site-packages (from sympy->torch>=1.12->botorch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch --upgrade\n",
        "%pip install gpytorch\n",
        "%pip install botorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NMY1hEY-jQyE"
      },
      "outputs": [],
      "source": [
        "from lcpfn.priors.fast_gp_mix import get_model\n",
        "\n",
        "\n",
        "def get_mcmc_model_variable_chains(x, y, hyperparameters, device, num_samples, warmup_steps, num_chains, obs=True):\n",
        "    from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
        "    import pyro\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    model, likelihood = get_model(x, y, hyperparameters, sample=False)\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    def pyro_model(x, y):\n",
        "        sampled_model = model.pyro_sample_from_prior()\n",
        "        output = sampled_model.likelihood(sampled_model(x))\n",
        "        if obs:\n",
        "            return pyro.sample(\"obs\", output, obs=y)\n",
        "\n",
        "    nuts_kernel = NUTS(pyro_model)\n",
        "    mcmc_run = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps, num_chains=num_chains)#num_chains=1)\n",
        "    mcmc_run.run(x, y)\n",
        "    model.pyro_load_from_samples(mcmc_run.get_samples()) # pyro.infer wie noah?\n",
        "    model.eval()\n",
        "\n",
        "    return model, likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oV8ryFE39hWU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/antonio14bernardes/.local/lib/python3.8/site-packages/pyro/infer/mcmc/api.py:499: UserWarning: num_chains=100 is more than available_cpu=7. Chains will be drawn sequentially.\n",
            "  warnings.warn(\n",
            "Warmup:   0%|          | 0/110 [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sample [0]:  63%|██████▎   | 69/110 [00:15,  4.67it/s, step size=9.11e-01, acc. prob=0.858]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 139750480158784",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-859502ab50d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cutoff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmcmc_model_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mcmc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpredictions_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood_10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcmc_model_10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a842c972cae4>\u001b[0m in \u001b[0;36mget_mcmc_model\u001b[0;34m(x, y, hyperparameters, device, num_samples, warmup_steps, num_chains, obs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnuts_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyro_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmcmc_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuts_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_chains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#num_chains=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmcmc_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyro_load_from_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcmc_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pyro.infer wie noah?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m ) -> Any:\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;31m# requires_grad\", which happens with `jit_compile` under PyTorch 1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mhook_w_logging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_logging_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             for sample in _gen_samples(\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36m_gen_samples\u001b[0;34m(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         hook(\n\u001b[1;32m    162\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/nuts.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 ):  # go to the right, start from the right leaf of current tree\n\u001b[0;32m--> 437\u001b[0;31m                     new_tree = self._build_tree(\n\u001b[0m\u001b[1;32m    438\u001b[0m                         \u001b[0mz_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                         \u001b[0mr_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/nuts.py\u001b[0m in \u001b[0;36m_build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhalf_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mz_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhalf_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_left_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         other_half_tree = self._build_tree(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/nuts.py\u001b[0m in \u001b[0;36m_build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# build the first half of tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         half_tree = self._build_tree(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/nuts.py\u001b[0m in \u001b[0;36m_build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtree_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             return self._build_basetree(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/nuts.py\u001b[0m in \u001b[0;36m_build_basetree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, energy_current)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_basetree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         z_new, r_new, z_grads, potential_energy = velocity_verlet(\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/ops/integrator.py\u001b[0m in \u001b[0;36mvelocity_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, num_steps, z_grads)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mr_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         z_next, r_next, z_grads, potential_energy = _single_step_verlet(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mz_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkinetic_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/ops/integrator.py\u001b[0m in \u001b[0;36m_single_step_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, z_grads)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# z(n+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mz_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpotential_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msite_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# r(n+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/ops/integrator.py\u001b[0m in \u001b[0;36mpotential_grad\u001b[0;34m(potential_fn, z)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mpotential_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpotential_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m# handle exceptions as defined in the exception registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/mcmc/util.py\u001b[0m in \u001b[0;36m_potential_fn\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mparams_constrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mcond_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_constrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         model_trace = poutine.trace(cond_model).get_trace(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m ) -> Any:\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m ) -> Any:\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m ) -> Any:\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a842c972cae4>\u001b[0m in \u001b[0;36mpyro_model\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpyro_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msampled_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyro_sample_from_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36mpyro_sample_from_prior\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mGPyTorch\u001b[0m \u001b[0mpriors\u001b[0m \u001b[0mregistered\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pyro_random_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pyro_sample_from_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36mto_pyro_random_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_pyro_random_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_random_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_random_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36mto_random_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mrandom_module_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_Random\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRandomModuleMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_module_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mnew_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_module_cls\u001b[0m  \u001b[0;31m# hack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# They tried\n",
        "# nsamples ∈ [100, 250, 500, 1000, 2000, 4000]\n",
        "# nwalkers ∈ [26, 50, 100]\n",
        "# burn-in ∈ [0, 50, 100, 500]\n",
        "# thin ∈ [1, 10, 100]\n",
        "\n",
        "hyperparameters = {'handmade': True} # Use the default handmade hyperparameters chosen by the authors\n",
        "device = 'cpu'\n",
        "num_samples = 100\n",
        "warmup_steps = 10\n",
        "num_chains = 100\n",
        "\n",
        "# For a cutoff of 10\n",
        "x = data_10['x']\n",
        "y = data_10['y']\n",
        "cutoff = data_10['cutoff']\n",
        "mcmc_model_10, likelihood_10 = get_mcmc_model_variable_chains(x[:cutoff].float(), y.flatten()[:cutoff].float(), hyperparameters, device, num_samples, warmup_steps, num_chains)\n",
        "with torch.no_grad():\n",
        "    predictions_10 = likelihood_10(mcmc_model_10(x[cutoff:].float()))\n",
        "    pred_mean_10 = predictions_10.mean.mean(0).squeeze()\n",
        "    pred_lower_10, pred_upper_10 = predictions_10.confidence_region()\n",
        "    pred_lower_10 = pred_lower_10.mean(0).squeeze()\n",
        "    pred_upper_10 = pred_upper_10.mean(0).squeeze()\n",
        "\n",
        "# For a cutoff of 20\n",
        "x = data_20['x']\n",
        "y = data_20['y']\n",
        "cutoff = data_20['cutoff']\n",
        "mcmc_model_20, likelihood_20 = get_mcmc_model_variable_chains(x[:cutoff].float(), y.flatten()[:cutoff].float(), hyperparameters, device, num_samples, warmup_steps, num_chains)\n",
        "with torch.no_grad():\n",
        "    predictions_20 = likelihood_20(mcmc_model_10(x[cutoff:].float()))\n",
        "    pred_mean_20 = predictions_20.mean.mean(0).squeeze()\n",
        "    pred_lower_20, pred_upper_20 = predictions_20.confidence_region()\n",
        "    pred_lower_20 = pred_upper_20.mean(0).squeeze()\n",
        "    pred_upper_20 = pred_upper_20.mean(0).squeeze()\n",
        "\n",
        "\n",
        "pred_mean_10, pred_mean_20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYStm3zpAP6N"
      },
      "outputs": [],
      "source": [
        "# Plotting the data and predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot predictions for cutoff = 10\n",
        "plt.plot(data_10['x'].flatten(), data_10['y'].flatten(), \"gray\", label=\"Data for Cutoff 10\")  # Actual data\n",
        "plt.plot(data_10['x'][data_10['cutoff']:].flatten(), pred_mean_10, \"blue\", label=\"Extrapolation for Cutoff 10\")\n",
        "plt.fill_between(data_10['x'][data_10['cutoff']:].flatten(), pred_lower_10, pred_upper_10, color=\"blue\", alpha=0.2, label=\"90% CI for Cutoff 10\")\n",
        "\n",
        "# Plot predictions for cutoff = 20\n",
        "plt.plot(data_20['x'].flatten(), data_20['y'].flatten(), \"lightgray\", label=\"Data for Cutoff 20\")  # Actual data\n",
        "plt.plot(data_20['x'][data_20['cutoff']:].flatten(), pred_mean_20, \"green\", label=\"Extrapolation for Cutoff 20\")\n",
        "plt.fill_between(data_20['x'][data_20['cutoff']:].flatten(), pred_lower_20, pred_upper_20, color=\"green\", alpha=0.2, label=\"90% CI for Cutoff 20\")\n",
        "\n",
        "# Plot cutoff lines\n",
        "plt.axvline(x=data_10['cutoff'], color='blue', linestyle='--', linewidth=0.5, label='Cutoff at 10')\n",
        "plt.axvline(x=data_20['cutoff'], color='green', linestyle='--', linewidth=0.5, label='Cutoff at 20')\n",
        "\n",
        "# Set plot limits, labels, title and legend\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"MCMC Model Extrapolation with Different Cutoffs\")\n",
        "plt.legend(loc=\"upper left\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
